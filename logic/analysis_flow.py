import streamlit as st
import io
from datetime import datetime
import matplotlib.pyplot as plt

# Import Steps
from logic.steps.step0_clarification import check_clarification
from logic.steps.step1_enhancement import enhance_prompt
from logic.steps.step2_code_gen import generate_code
from logic.steps.step3_execution import execute_and_fix
from logic.steps.step4_reflection import check_logic_and_fix
from logic.steps.step6_insight import generate_insights

def process_user_query(prompt, client, model_choice, df, data_schema_info, enable_clarification, use_history):
    """
    è™•ç†ä½¿ç”¨è€…æŸ¥è©¢çš„æ ¸å¿ƒé‚è¼¯æµç¨‹ (Orchestrator)
    """
    
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.status("AI æ•¸æ“šåˆ†æå¸«æ­£åœ¨è™•ç†ä¸­...") as status:
            try:
                # --- [Step 0: å•é¡Œæª¢æŸ¥èˆ‡æ¾„æ¸…] ---
                status.update(label="Step 0/6: æª¢æŸ¥å•é¡Œæ˜¯å¦éœ€è¦æ¾„æ¸…...")
                clarification_data = check_clarification(client, model_choice, prompt, data_schema_info, enable_clarification)
                
                if clarification_data:
                    # è¨­å®šæ¾„æ¸…ç‹€æ…‹
                    st.session_state.awaiting_clarification = True
                    st.session_state.clarification_data = clarification_data
                    st.session_state.original_prompt = prompt

                    # é¡¯ç¤ºæ¾„æ¸…å•é¡Œ
                    st.markdown(f"### ğŸ¤” {clarification_data['question']}")
                    st.info("è«‹åœ¨ä¸‹æ–¹è¼¸å…¥æ¡†ä¸­é¸æ“‡ä»¥ä¸‹é¸é …ä¹‹ä¸€ï¼ˆè¼¸å…¥é¸é …ç·¨è™Ÿæˆ–å®Œæ•´æè¿°ï¼‰ï¼Œæˆ–ç›´æ¥è¼¸å…¥æ‚¨çš„è£œå……èªªæ˜ï¼š")

                    options_text = ""
                    for i, option in enumerate(clarification_data['options'], 1):
                        option_line = f"**{i}.** {option}"
                        st.markdown(option_line)
                        options_text += f"{i}. {option}\n"

                    # å„²å­˜åŠ©æ‰‹å›æ‡‰åˆ°æ­·å²
                    clarification_msg = f"### ğŸ¤” {clarification_data['question']}\n\n"
                    clarification_msg += "è«‹é¸æ“‡ä»¥ä¸‹é¸é …ä¹‹ä¸€ï¼Œæˆ–ç›´æ¥æä¾›è£œå……èªªæ˜ï¼š\n\n"
                    clarification_msg += options_text

                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": clarification_msg,
                        "figures": []
                    })

                    status.update(label="ç­‰å¾…æ‚¨çš„è£œå……è³‡è¨Š...", state="complete")
                    return # ä¸­æ–·åŸ·è¡Œ

                # --- [Step 1: è½‰åŒ–ä½¿ç”¨è€…å•é¡Œ] ---
                status.update(label="Step 1/6: æ­£åœ¨é‡æ¸…æ‚¨çš„å•é¡Œ...")
                enhanced_prompt, relevant_topics, needs_court_info, _ = enhance_prompt(client, model_choice, prompt)
                print(f"Enhanced Prompt: {enhanced_prompt}")
                print(f"Topics: {relevant_topics}")

                # --- [Step 2: ç”Ÿæˆåˆ†æç¨‹å¼ç¢¼] ---
                status.update(label="Step 2/6: æ­£åœ¨ç”Ÿæˆåˆ†æç¨‹å¼ç¢¼...")
                code_to_execute, conversation = generate_code(
                    client, model_choice, enhanced_prompt, data_schema_info, 
                    relevant_topics, needs_court_info, use_history
                )

                # --- [Step 3: åŸ·è¡Œç¨‹å¼] ---
                status.update(label="Step 3/6: æ­£åœ¨åŸ·è¡Œç¨‹å¼ç¢¼...")
                
                if code_to_execute:
                    # status.update is passed as a wrapper lambda to match signature expected by step3
                    def update_status(label, state=None):
                        status.update(label=label, state=state)
                        
                    success, code_to_execute, execution_output, exec_globals, last_error = execute_and_fix(
                        client, model_choice, code_to_execute, df, conversation, status_updater=update_status
                    )
                    
                    if not success:
                        raise last_error

                    # --- [Step 4: é‚è¼¯åé¥‹èˆ‡ä¿®æ­£] ---
                    status.update(label="Step 4/6: AI æ­£åœ¨æª¢æŸ¥åˆ†æçµæœçš„é‚è¼¯æ€§...")
                    
                    code_to_execute, execution_output, summary_info, final_figs = check_logic_and_fix(
                        client, model_choice, prompt, code_to_execute, execution_output, exec_globals, df, status_updater=update_status
                    )
                else:
                    execution_output = ""
                    summary_info = {}
                    final_figs = []

                # --- [Step 5: é¡¯ç¤ºåˆ†æå…§å®¹ (UI)] ---
                if not summary_info:
                    summary_info = {
                        "æç¤º": "AI æœªè¼¸å‡ºå¯ä¾›åˆ†æçš„çµ±è¨ˆè®Šæ•¸ï¼Œè«‹æ ¹æ“šåœ–è¡¨èˆ‡æå•é‚è¼¯ç”Ÿæˆæ´å¯Ÿã€‚"
                    }

                if code_to_execute:
                    with st.expander("ğŸ§  æŸ¥çœ‹ AI å„ªåŒ–å¾Œçš„æå•é‚è¼¯ (Step 1)", expanded=False):
                        st.markdown(f"**å„ªåŒ–å°å¼• (Enhanced Prompt):**\n{enhanced_prompt}")

                    with st.expander("ğŸ§¾ æŸ¥çœ‹ AI ç”Ÿæˆçš„ç¨‹å¼ç¢¼ (æœ€çµ‚ç‰ˆ)", expanded=False):
                        # ç¢ºä¿ code_to_execute æ˜¯å­—ä¸²
                        st.code(str(code_to_execute), language="python")

                if final_figs:
                    for i, fig in enumerate(final_figs):
                        st.pyplot(fig)
                        buf = io.BytesIO()
                        fig.savefig(buf, format="png", dpi=300, bbox_inches="tight")
                        buf.seek(0)
                        st.download_button(
                            f"ğŸ“¥ ä¸‹è¼‰åœ–è¡¨ {i+1}",
                            data=buf,
                            file_name=f"ç¾½çƒåˆ†æ_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}.png",
                            mime="image/png",
                            key=f"download_new_{i}"
                        )
                elif not execution_output:
                    st.warning("âš ï¸ AI æ²’æœ‰è¼¸å‡ºåœ–è¡¨ä¹Ÿæ²’æœ‰æ–‡å­—è¼¸å‡º (å¯èƒ½æ˜¯è³‡æ–™ç¯©é¸å¾Œç‚ºç©ºï¼Œå»ºè­°æª¢æŸ¥çƒå“¡åç¨±æ˜¯å¦æ­£ç¢º)ã€‚")

                # --- [Step 6: ç”Ÿæˆæ•¸æ“šæ´å¯Ÿ] ---
                status.update(label="Step 5/6: æ­£åœ¨æ’°å¯«æ•¸æ“šæ´å¯Ÿ...")
                summary_text = ""
                st.markdown("### ğŸ“Š æ•¸æ“šæ´å¯Ÿ")
                
                if execution_output:
                    st.markdown("#### ğŸ“‹ ç¨‹å¼åŸ·è¡Œçµæœ")
                    st.code(execution_output, language="text")
                    st.divider()

                summary_text = generate_insights(client, model_choice, prompt, execution_output, summary_info)
                st.markdown(summary_text)

                # --- [Step 7: å„²å­˜è‡³æ­·å²] ---
                code_block_for_history = f"```python\n{code_to_execute}\n```" if code_to_execute else ""
                final_content_for_history = (
                    f"{code_block_for_history}\n\n"
                    f"---\n"
                    f"### ğŸ“Š æ•¸æ“šæ´å¯Ÿ\n"
                    f"{summary_text}"
                )
                
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": final_content_for_history.strip(),
                    "figures": final_figs,
                    "enhanced_prompt": enhanced_prompt
                })

                status.update(label="åˆ†æå®Œæˆï¼", state="complete")

            except Exception as e:
                status.update(label="åˆ†æå¤±æ•—", state="error")
                st.error(f"âŒ éŒ¯èª¤: {e}")
                # Print stack trace to console for debugging
                import traceback
                traceback.print_exc()
                
                st.session_state.messages.append({
                    "role": "assistant", "content": str(e), "figure": None
                })
