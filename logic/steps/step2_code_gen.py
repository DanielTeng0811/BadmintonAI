import streamlit as st
from config.prompts import create_system_prompt
from utils.data_loader import get_filtered_schema_string
from utils.app_utils import log_llm_interaction, load_court_info

def generate_code(client, model_choice, enhanced_prompt, data_schema_info, relevant_topics, needs_court_info, use_history):
    """
    Step 2: æ ¹æ“š Enhanced Prompt ç”Ÿæˆåˆ†æç¨‹å¼ç¢¼ã€‚
    
    Returns:
        code_to_execute (str or None)
        conversation (list) - å®Œæ•´çš„å°è©±ç´€éŒ„ï¼Œä¾›å¾ŒçºŒä¿®æ­£ä½¿ç”¨
    """
    
    court_place_info = load_court_info()
    
    # [Dynamic Schema Injection]: æ ¹æ“š Topics è¼‰å…¥å°æ‡‰æ¬„ä½
    dynamic_col_defs = get_filtered_schema_string(relevant_topics)
    system_prompt = create_system_prompt(data_schema_info, dynamic_col_defs)
    
    # å‹•æ…‹æ³¨å…¥å ´åœ°è³‡è¨Š
    if needs_court_info and court_place_info:
        system_prompt += f"\n\n**å ´åœ°ä½ç½®åƒè€ƒè³‡è¨Š (Court Grid Definitions):**\n{court_place_info}\n"

    # [è¦–è¦ºåŒ–æŒ‡å°åŸå‰‡]
    system_prompt += """
    \n**æœ€ä½³å¯¦è¸:**
    1. å€åˆ†é€£çºŒæ•¸å€¼(Float)èˆ‡é¡åˆ¥ã€‚åº§æ¨™å‹¿ç›´æ¥ groupbyã€‚
    2. è»¸æ¨™ç±¤é¿å…å¤§é‡æµ®é»æ•¸ã€‚
    3. ç¹ªåœ–å‰æª¢æŸ¥ `if len(filtered_df) > 0:`ã€‚
    """

    conversation = [{"role": "system", "content": system_prompt}]
    
    # [åŠ å…¥æ­·å²è¨Šæ¯]
    if use_history and len(st.session_state.messages) > 1:
        valid_history = []
        # å¾å€’æ•¸ç¬¬äºŒå‰‡è¨Šæ¯é–‹å§‹å¾€å›çœ‹ (æ’é™¤ç•¶å‰æœ€æ–°è¨Šæ¯)
        for m in reversed(st.session_state.messages[:-1]):
            if not m.get("tracked", True): 
                break
                
            if m.get("content") and "ğŸ¤”" not in m.get("content", ""):
                valid_history.insert(0, {"role": m["role"], "content": m["content"]})
        
        # åƒ…ä¿ç•™æœ€å¾Œ 4 è¼ª
        recent_history = valid_history[-8:]
        conversation.extend(recent_history)
    
    conversation.append({"role": "user", "content": enhanced_prompt})

    response = client.chat.completions.create(
        model=model_choice, messages=conversation
    )
    ai_response = response.choices[0].message.content
    log_llm_interaction("Step 2: Code Generation", conversation, ai_response)

    # å–å‡º Python code
    code_to_execute = None
    if "```python" in ai_response:
        start = ai_response.find("```python") + len("```python\n")
        end = ai_response.rfind("```")
        code_to_execute = ai_response[start:end].strip()

    return code_to_execute, conversation
