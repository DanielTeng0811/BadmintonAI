import streamlit as st
import os
import io
import sys
from contextlib import redirect_stdout
import platform
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
import zipfile
import matplotlib.pyplot as plt # ç¢ºä¿ matplotlib è¢«å°å…¥
import seaborn as sns # å¼•å…¥ seaborn æä¾›æ›´å¤šç¹ªåœ–é¸æ“‡ï¼Œä½†ä¸å¼·åˆ¶ä½¿ç”¨

# è‡ªè¨‚æ¨¡çµ„ (è«‹ç¢ºä¿ config/prompts.py è£¡é¢æ²’æœ‰ circular import)
from config.prompts import create_system_prompt
from utils.data_loader import load_all_data
from utils.ai_client import initialize_client

# --- åˆå§‹è¨­å®šèˆ‡ç’°å¢ƒè®Šæ•¸è¼‰å…¥ ---
load_dotenv()

# è¨­å®šé é¢
st.set_page_config(
    page_title="ç¾½çƒ AI æ•¸æ“šåˆ†æå¸«",
    page_icon="ğŸ¸",
    layout="wide"
)

# --- è¼”åŠ©å‡½æ•¸ï¼šå®‰å…¨è®€å– API Key ---
def get_api_key(key_name):
    """
    å¾ç’°å¢ƒè®Šæ•¸æˆ– Streamlit Secrets å®‰å…¨è®€å– API Keyã€‚
    """
    # å„ªå…ˆå¾ .env ç’°å¢ƒè®Šæ•¸è®€å–
    env_value = os.getenv(key_name, "")
    if env_value:
        return env_value

    # å¦‚æœç’°å¢ƒè®Šæ•¸æ²’æœ‰ï¼Œå˜—è©¦å¾ Streamlit Secrets è®€å–
    try:
        if hasattr(st, 'secrets') and st.secrets:
            return st.secrets.get(key_name, "")
    except Exception:
        pass

    return ""

# --- è³‡æ–™è¼‰å…¥ ---
df, data_schema_info, column_definitions_info = load_all_data()

# --- Streamlit UI ---
st.title("ğŸ¸ ç¾½çƒ AI æ•¸æ“šåˆ†æå¸«")
st.markdown("#### é€éè‡ªç„¶èªè¨€ï¼Œç›´æ¥ç”Ÿæˆæ•¸æ“šåˆ†æåœ–è¡¨")

# å´é‚Šæ¬„
with st.sidebar:
    st.header("âš™ï¸ API è¨­å®š")
    api_mode = st.selectbox("API æ¨¡å¼", ["Gemini", "OpenAI å®˜æ–¹", "äº¤å¤§ä¼ºæœå™¨"], index=0)
    api_key_env_var = "GEMINI_API_KEY" if api_mode == "Gemini" else "OPENAI_API_KEY"

    # ä½¿ç”¨ get_api_key å‡½æ•¸å®‰å…¨è®€å– API Key
    default_api_key = get_api_key(api_key_env_var)

    api_key_input = st.text_input(
        f"{api_mode} API Key",
        value=default_api_key,
        type="password",
        help="ğŸ’¡ æœ¬åœ°é–‹ç™¼ï¼šå¾ .env è®€å– | é›²ç«¯éƒ¨ç½²ï¼šå¾ Streamlit Secrets è®€å–"
    )

    if api_mode == "Gemini":
        model_choice = st.selectbox("é¸æ“‡æ¨¡å‹",["gemini-2.0-flash", "gemini-1.5-pro", "gemini-1.5-flash"], index=0)
    else:
        model_choice = st.selectbox("é¸æ“‡æ¨¡å‹", ["gpt-4o-mini", "gpt-4o"], index=0)

    st.divider()

    # å¤šè¼ªå•ç­”é–‹é—œ
    enable_clarification = st.checkbox("å•Ÿç”¨å¤šè¼ªå•ç­”ï¼ˆå•é¡Œä¸æ˜ç¢ºæ™‚æœƒä¸»å‹•è©¢å•ï¼‰", value=False)

    st.divider()
    st.markdown("#### ç¯„ä¾‹å•é¡Œ")
    st.info("""
    - çƒå“¡ A çš„å„çƒç¨®åˆ†ä½ˆæ˜¯æ€éº¼æ¨£çš„ï¼Ÿè«‹ç”¨åœ“é¤…åœ–å‘ˆç¾ã€‚
    - å“ªå€‹è½é» (`landing_zone`) çš„çƒæœ€å¸¸å‡ºç¾ï¼Ÿè«‹ç”¨é•·æ¢åœ–è¡¨ç¤ºã€‚
    - å„çƒå“¡ (`player`) çš„æ®ºçƒ (`smash`) æ¬¡æ•¸æ¯”è¼ƒã€‚
    - èª°æ˜¯å¤±èª¤ç‹ï¼Ÿè«‹çµ±è¨ˆå„çƒå“¡çš„å¤±èª¤æ¬¡æ•¸ã€‚
    """)
    st.divider()

    # --- ZIP åŒ¯å‡ºåŠŸèƒ½ ---
    zip_buffer = io.BytesIO()
    has_messages = "messages" in st.session_state and st.session_state.messages
    if has_messages:
        with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zip_f:
            markdown_content = f"# ğŸ¸ ç¾½çƒ AI æ•¸æ“šåˆ†æå¸« - åˆ†æå ±å‘Š\n"
            markdown_content += f"**å„²å­˜æ™‚é–“:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n\n"
            chart_counter = 0
            for message in st.session_state.messages:
                role_emoji = "ğŸ‘¤" if message["role"] == "user" else "ğŸ¤–"
                role_title = "ä½¿ç”¨è€…æå•" if message["role"] == "user" else "AI åˆ†æå¸«å›è¦†"
                content_to_save = message["content"]
                
                # åœ¨å„²å­˜æ™‚ï¼Œå°‡ç¨‹å¼ç¢¼å€å¡Šä¿ç•™
                markdown_content += f"### {role_emoji} {role_title}\n{content_to_save.strip()}\n\n"
                
                figures = message.get("figures", [])
                if not figures and message.get("figure"):
                    figures = [message["figure"]]

                for fig in figures:
                    chart_counter += 1
                    chart_filename = f"chart_{chart_counter}.png"
                    img_buffer = io.BytesIO()
                    fig.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                    img_buffer.seek(0)
                    zip_f.writestr(chart_filename, img_buffer.getvalue())
                    markdown_content += f"![ç”¢ç”Ÿçš„åœ–è¡¨ {chart_counter}]({chart_filename})\n\n"
                markdown_content += "---\n\n"
            zip_f.writestr("åˆ†æå ±å‘Š.md", markdown_content.encode('utf-8'))

    st.download_button(
        label="ğŸ’¾ ä¸‹è¼‰åˆ†æå ±å‘Š (ZIP)",
        data=zip_buffer.getvalue(),
        file_name=f"ç¾½çƒåˆ†æå ±å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
        mime="application/zip",
        disabled=not has_messages
    )

    if st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±"):
        st.session_state.messages = []
        st.rerun()

# åˆå§‹åŒ– client èˆ‡å°è©±
client = initialize_client(api_mode, api_key_input)
if "messages" not in st.session_state:
    st.session_state.messages = []

# åˆå§‹åŒ–å¤šè¼ªå•ç­”ç‹€æ…‹
if "awaiting_clarification" not in st.session_state:
    st.session_state.awaiting_clarification = False
if "clarification_data" not in st.session_state:
    st.session_state.clarification_data = None
if "original_prompt" not in st.session_state:
    st.session_state.original_prompt = ""

# é¡¯ç¤ºæ­·å²
for idx, message in enumerate(st.session_state.messages):
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        figures = message.get("figures", [])
        if not figures and message.get("figure"):
            figures = [message["figure"]]

        for fig_idx, fig in enumerate(figures):
            st.pyplot(fig)
            buf = io.BytesIO()
            fig.savefig(buf, format='png', dpi=300, bbox_inches='tight')
            buf.seek(0)
            st.download_button(
                label=f"ğŸ“¥ ä¸‹è¼‰åœ–è¡¨ {fig_idx + 1}",
                data=buf,
                file_name=f"ç¾½çƒåˆ†æ_{idx}_{fig_idx}_{datetime.now().strftime('%Y%m%d')}.png",
                mime="image/png",
                key=f"download_history_{idx}_{fig_idx}",
            )

# --- ä¸»å°è©±æµç¨‹ ---
if prompt := st.chat_input("è«‹è¼¸å…¥ä½ çš„æ•¸æ“šåˆ†æå•é¡Œ..."):
    if df is None:
        st.error("âŒ æ‰¾ä¸åˆ° 'all_dataset.csv'ã€‚")
    elif not api_key_input:
        st.error("âš ï¸ è«‹è¼¸å…¥ API Keyã€‚")
    else:
        # === è™•ç†æ¾„æ¸…å›æ‡‰ ===
        skip_clarification = False
        if st.session_state.awaiting_clarification:
            # ä½¿ç”¨è€…å·²ç¶“é¸æ“‡äº†é¸é …æˆ–æä¾›è£œå……èªªæ˜
            user_answer = prompt.strip()
            clarification_data = st.session_state.clarification_data

            # æª¢æŸ¥æ˜¯å¦æ˜¯é¸é …ç·¨è™Ÿ
            if user_answer.isdigit() and clarification_data:
                option_index = int(user_answer) - 1
                if 0 <= option_index < len(clarification_data.get('options', [])):
                    user_answer = clarification_data['options'][option_index]

            # çµ„åˆå®Œæ•´å•é¡Œ
            full_prompt = f"{st.session_state.original_prompt}\nè£œå……èªªæ˜: {user_answer}"

            # è¨˜éŒ„ä½¿ç”¨è€…çš„è£œå……å›æ‡‰
            st.session_state.messages.append({"role": "user", "content": prompt})

            # é‡ç½®æ¾„æ¸…ç‹€æ…‹
            st.session_state.awaiting_clarification = False
            st.session_state.clarification_data = None

            # ä½¿ç”¨å®Œæ•´å•é¡Œé€²è¡Œåˆ†æ
            prompt = full_prompt
            skip_clarification = True
        else:
            st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            # ä½¿ç”¨ st.status ä¾†é¡¯ç¤ºå¤šæ­¥é©Ÿé€²ç¨‹
            with st.status("AI æ•¸æ“šåˆ†æå¸«æ­£åœ¨è™•ç†ä¸­...") as status:
                try:
                    # --- [Step 0: å•é¡Œæª¢æŸ¥èˆ‡æ¾„æ¸…] ---
                    if not skip_clarification and enable_clarification:
                        status.update(label="Step 0/6: æª¢æŸ¥å•é¡Œæ˜¯å¦éœ€è¦æ¾„æ¸…...")

                        import json
                        clarification_check_prompt = f"""
                        ä½ æ˜¯ä¸€å€‹å•é¡Œæª¢æŸ¥åŠ©æ‰‹ã€‚è«‹åˆ¤æ–·ä½¿ç”¨è€…çš„å•é¡Œæ˜¯å¦**è¶³å¤ æ˜ç¢º**å¯ä»¥ç›´æ¥é€²è¡Œæ•¸æ“šåˆ†æã€‚

                        ä½¿ç”¨è€…å•é¡Œ: "{prompt}"

                        å¯ç”¨çš„è³‡æ–™æ¬„ä½:
                        {data_schema_info}

                        **åˆ¤æ–·æ¨™æº–:**
                        - å¦‚æœå•é¡Œç¼ºå°‘é—œéµè³‡è¨Šï¼ˆä¾‹å¦‚ï¼šæ²’æŒ‡å®šçƒå“¡åç¨±ã€æ™‚é–“ç¯„åœæ¨¡ç³Šã€çµ±è¨ˆæ–¹å¼ä¸æ˜ç¢ºã€æ¯”è¼ƒå°è±¡ä¸æ¸…æ¥šï¼‰
                        - å¦‚æœå•é¡Œæœ‰å¤šç¨®åˆç†è§£è®€æ–¹å¼
                        - å¦‚æœä½¿ç”¨è€…ä½¿ç”¨äº†ä»£åè©ï¼ˆä¾‹å¦‚ã€Œä»–ã€ã€ã€Œé€™å€‹ã€ã€ã€Œé‚£å ´æ¯”è³½ã€ï¼‰ä½†ä¸Šä¸‹æ–‡ä¸æ¸…æ¥š

                        å‰‡éœ€è¦æ¾„æ¸…ã€‚

                        **è¼¸å‡ºæ ¼å¼ï¼ˆäºŒé¸ä¸€ï¼‰:**
                        1. å¦‚æœå•é¡Œå·²ç¶“è¶³å¤ æ˜ç¢ºï¼Œåªè¼¸å‡º: CLEAR
                        2. å¦‚æœéœ€è¦æ¾„æ¸…ï¼Œè¼¸å‡º JSON æ ¼å¼ï¼ˆä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰:
                        {{
                        "need_clarification": true,
                        "question": "è«‹å•æ‚¨æƒ³è¦...",
                        "options": ["é¸é …1çš„å®Œæ•´æè¿°", "é¸é …2çš„å®Œæ•´æè¿°", "é¸é …3çš„å®Œæ•´æè¿°"]
                        }}
                        """

                        clarification_response = client.chat.completions.create(
                            model=model_choice,
                            messages=[{"role": "user", "content": clarification_check_prompt}],
                            temperature=0.3
                        )
                        clarification_content = clarification_response.choices[0].message.content.strip()

                        # æª¢æŸ¥æ˜¯å¦éœ€è¦æ¾„æ¸…
                        if "CLEAR" not in clarification_content:
                            try:
                                # æå– JSON
                                json_str = clarification_content
                                if "```json" in clarification_content:
                                    start = clarification_content.find("```json") + 7
                                    end = clarification_content.find("```", start)
                                    json_str = clarification_content[start:end].strip()
                                elif "```" in clarification_content:
                                    start = clarification_content.find("```") + 3
                                    end = clarification_content.find("```", start)
                                    json_str = clarification_content[start:end].strip()

                                clarification_data = json.loads(json_str)

                                if clarification_data.get("need_clarification"):
                                    # è¨­å®šæ¾„æ¸…ç‹€æ…‹
                                    st.session_state.awaiting_clarification = True
                                    st.session_state.clarification_data = clarification_data
                                    st.session_state.original_prompt = prompt

                                    # é¡¯ç¤ºæ¾„æ¸…å•é¡Œ
                                    st.markdown(f"### ğŸ¤” {clarification_data['question']}")
                                    st.info("è«‹åœ¨ä¸‹æ–¹è¼¸å…¥æ¡†ä¸­é¸æ“‡ä»¥ä¸‹é¸é …ä¹‹ä¸€ï¼ˆè¼¸å…¥é¸é …ç·¨è™Ÿæˆ–å®Œæ•´æè¿°ï¼‰ï¼Œæˆ–ç›´æ¥è¼¸å…¥æ‚¨çš„è£œå……èªªæ˜ï¼š")

                                    options_text = ""
                                    for i, option in enumerate(clarification_data['options'], 1):
                                        option_line = f"**{i}.** {option}"
                                        st.markdown(option_line)
                                        options_text += f"{i}. {option}\n"

                                    # å„²å­˜åŠ©æ‰‹å›æ‡‰åˆ°æ­·å²
                                    clarification_msg = f"### ğŸ¤” {clarification_data['question']}\n\n"
                                    clarification_msg += "è«‹é¸æ“‡ä»¥ä¸‹é¸é …ä¹‹ä¸€ï¼Œæˆ–ç›´æ¥æä¾›è£œå……èªªæ˜ï¼š\n\n"
                                    clarification_msg += options_text

                                    st.session_state.messages.append({
                                        "role": "assistant",
                                        "content": clarification_msg,
                                        "figures": []
                                    })

                                    status.update(label="ç­‰å¾…æ‚¨çš„è£œå……è³‡è¨Š...", state="complete")
                                    st.stop()

                            except json.JSONDecodeError:
                                # JSON è§£æå¤±æ•—ï¼Œç¹¼çºŒæ­£å¸¸æµç¨‹
                                pass

                    # --- [Step 1: è½‰åŒ–ä½¿ç”¨è€…å•é¡Œ] ---
                    status.update(label="Step 1/6: æ­£åœ¨é‡æ¸…æ‚¨çš„å•é¡Œ...")
                    
                    enhancement_system_prompt = f"""
                    ä½ æ˜¯ä¸€å€‹è¼”åŠ©ç³»çµ±ï¼Œä½ çš„ä»»å‹™æ˜¯å°‡ä½¿ç”¨è€…çš„ç°¡çŸ­æ•¸æ“šåˆ†æå•é¡Œï¼Œè½‰åŒ–ç‚ºä¸€å€‹æ›´æ¸…æ™°ã€æ›´å®Œæ•´ã€æ›´å…·é«”çš„æ•¸æ“šåˆ†æå•é¡Œï¼Œå¿…é ˆè€ƒæ…®ä½¿ç”¨è€…æ‰€æœ‰æ–¹é¢çš„å¯èƒ½ï¼ŒåŠæ•¸æ“šä¸­æ‰€æœ‰æ¬„ä½çš„é—œè¯æ€§ã€‚
                    é€™å€‹æè¿°å°‡è¢«äº¤çµ¦å¦ä¸€å€‹ AI (Python ç¨‹å¼ç¢¼ç”Ÿæˆå™¨) ä¾†åŸ·è¡Œã€‚
                    
                    ä½ å¿…é ˆè€ƒæ…®ä»¥ä¸‹çš„è³‡æ–™åº« schemaï¼š
                    {data_schema_info}
                    
                    ä½ çš„è¼¸å‡º**åªèƒ½**åŒ…å«è½‰åŒ–å¾Œç²¾ç°¡çš„ç¹é«”ä¸­æ–‡å•é¡Œæ•˜è¿°ï¼Œä¸è¦æœ‰ä»»ä½•å‰è¨€ã€å¾Œèªæˆ–è§£é‡‹ã€‚
                    """
                    
                    enhancement_response = client.chat.completions.create(
                        model=model_choice,
                        messages=[
                            {"role": "system", "content": enhancement_system_prompt},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=0.2
                    )
                    enhanced_prompt = enhancement_response.choices[0].message.content.strip()
                    print(f"Enhanced Prompt: {enhanced_prompt}")

                    # --- [Step 2: ç”Ÿæˆåˆ†æç¨‹å¼ç¢¼] ---
                    status.update(label="Step 2/6: æ­£åœ¨ç”Ÿæˆåˆ†æç¨‹å¼ç¢¼...")
                    system_prompt = create_system_prompt(data_schema_info, column_definitions_info)
                    
                    # [ä¿®æ”¹é»]ï¼šæ³¨å…¥é€šç”¨ä¸”ç©©å¥çš„è¦–è¦ºåŒ–æŒ‡å°åŸå‰‡ï¼Œè€Œéå¼·åˆ¶ç‰¹å®šæ–¹æ³•
                    system_prompt += """
                    \n**æ•¸æ“šåˆ†æèˆ‡è¦–è¦ºåŒ–æœ€ä½³å¯¦è¸ (Analysis & Visualization Best Practices):**
                    1. **è³‡æ–™å‹æ…‹æ„è­˜ (Data Type Awareness)**:
                       - åœ¨å½™æ•´æˆ–ç¹ªåœ–å‰ï¼Œè«‹ç¢ºèªæ¬„ä½æ˜¯ã€Œé€£çºŒæ•¸å€¼ (Float)ã€é‚„æ˜¯ã€Œé›¢æ•£é¡åˆ¥ (Category/Int)ã€ã€‚
                       - è‹¥æ˜¯å°ã€Œé€£çºŒåº§æ¨™ (Float)ã€é€²è¡Œåˆ†æ (å¦‚è½é»ã€è·‘å‹•ä½ç½®)ï¼Œ**åš´ç¦**ç›´æ¥ä½¿ç”¨ `groupby` è¨ˆç®—æ¬¡æ•¸ï¼Œå› ç‚ºåº§æ¨™å¹¾ä¹ä¸æœƒå®Œå…¨é‡è¤‡ï¼Œé€™æœƒå°è‡´åœ–è¡¨ç©ºç™½æˆ–åº§æ¨™è»¸å´©æ½°ã€‚
                    2. **åº§æ¨™è»¸å¯è®€æ€§ (Label Readability)**:
                       - é¿å…å°‡å¤§é‡æµ®é»æ•¸ç›´æ¥ä½œç‚ºè»¸æ¨™ç±¤ã€‚
                    3. **è³‡æ–™é‡æª¢æŸ¥ (Data Integrity)**:
                       - åœ¨ç¹ªåœ–å‰ï¼Œå‹™å¿…æª¢æŸ¥ç¯©é¸å¾Œçš„ DataFrame æ˜¯å¦ç‚ºç©º (`if len(filtered_df) > 0: ...`)ã€‚
                    """

                    conversation = [{"role": "system", "content": system_prompt}]
                    if len(st.session_state.messages) > 1:
                        for m in st.session_state.messages[:-1]:
                            # è·³éæ¾„æ¸…ç›¸é—œçš„å°è©±ï¼ˆåŒ…å« ğŸ¤” emoji çš„è¨Šæ¯ï¼‰
                            if m.get("content") and "ğŸ¤”" not in m.get("content", ""):
                                conversation.append({"role": m["role"], "content": m["content"]})
                    
                    conversation.append({"role": "user", "content": enhanced_prompt})

                    response = client.chat.completions.create(
                        model=model_choice, messages=conversation
                    )
                    ai_response = response.choices[0].message.content

                    # å–å‡º Python code
                    code_to_execute = None
                    if "```python" in ai_response:
                        start = ai_response.find("```python") + len("```python\n")
                        end = ai_response.rfind("```")
                        code_to_execute = ai_response[start:end].strip()

                    # --- [Step 3: åŸ·è¡Œç¨‹å¼ (Runtime Error Fix Loop)] ---
                    status.update(label="Step 3/6: æ­£åœ¨åŸ·è¡Œç¨‹å¼ç¢¼...")
                    
                    final_figs = []
                    summary_info = {}
                    exec_globals = {} # åˆå§‹åŒ–ç’°å¢ƒè®Šæ•¸
                    
                    if code_to_execute:
                        execution_output = ""
                        max_retries = 3
                        retry_count = 0
                        success = False
                        last_error = None
                        
                        # è¿´åœˆ 1: è™•ç†èªæ³•/åŸ·è¡ŒéŒ¯èª¤ (Syntax/Runtime Errors)
                        while retry_count <= max_retries:
                            try:
                                # é‡è¦ï¼šæ¯æ¬¡åŸ·è¡Œå‰æ¸…é™¤ Matplotlib ç‹€æ…‹ï¼Œé¿å…ä¸Šä¸€å¼µåœ–æ®˜ç•™æˆ–å¹²æ“¾
                                plt.close('all')
                                
                                # æº–å‚™åŸ·è¡Œç’°å¢ƒï¼Œç¢ºä¿ df å­˜åœ¨
                                # åŠ å…¥ sns åˆ°åŸ·è¡Œç’°å¢ƒï¼Œæä¾›æ›´å¤šå½ˆæ€§
                                exec_globals = {
                                    "pd": pd, 
                                    "df": df.copy(), 
                                    "st": st, 
                                    "platform": platform, 
                                    "io": io, 
                                    "plt": plt,
                                    "sns": sns 
                                }
                                f = io.StringIO()
                                with redirect_stdout(f):
                                    exec(code_to_execute, exec_globals)
                                execution_output = f.getvalue()
                                success = True
                                break 
                            except Exception as e:
                                retry_count += 1
                                last_error = e
                                status.update(label=f"Step 3/6: ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤ï¼ŒAI æ­£åœ¨ä¿®å¾©èªæ³• (å˜—è©¦ {retry_count}/{max_retries})...", state="running")
                                
                                conversation.append({"role": "assistant", "content": f"```python\n{code_to_execute}\n```"})
                                error_feedback = f"åŸ·è¡Œä¸Šè¿°ç¨‹å¼ç¢¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}ã€‚è«‹ä¿®æ­£éŒ¯èª¤ä¸¦é‡æ–°è¼¸å‡ºå®Œæ•´ç¨‹å¼ç¢¼ (åŒ…å«å¿…è¦çš„ import)ã€‚"
                                conversation.append({"role": "user", "content": error_feedback})
                                
                                correction_response = client.chat.completions.create(model=model_choice, messages=conversation)
                                ai_correction = correction_response.choices[0].message.content
                                
                                if "```python" in ai_correction:
                                    start = ai_correction.find("```python") + len("```python\n")
                                    end = ai_correction.rfind("```")
                                    code_to_execute = ai_correction[start:end].strip() # æ›´æ–°ä»£ç¢¼

                        if not success:
                            raise last_error

                        # --- æå–è®Šæ•¸ (ä¾›ä¸‹ä¸€æ­¥é‚è¼¯æª¢æŸ¥ä½¿ç”¨) ---
                        ignore_list = ['df', 'pd', 'st', 'platform', 'io', 'fig', 'np', 'plt', 'sns']
                        
                        # æª¢æŸ¥ç”Ÿæˆçš„åœ–è¡¨æ•¸é‡
                        created_figs = [plt.figure(n) for n in plt.get_fignums()]
                        if not created_figs and "fig" in exec_globals:
                             created_figs = [exec_globals["fig"]]
                        
                        summary_info["_generated_figures_count"] = len(created_figs)

                        for name, val in exec_globals.items():
                            if name.startswith('_') or name in ignore_list: continue
                            
                            if isinstance(val, (int, float, str, bool)):
                                summary_info[name] = val
                            elif isinstance(val, (pd.DataFrame, pd.Series)):
                                # å¼·åˆ¶è®“ LLM çŸ¥é“è³‡æ–™æ˜¯ç©ºçš„
                                if val.empty:
                                    summary_info[name] = "âš ï¸ Empty DataFrame/Series (0 rows)"
                                else:
                                    # å¦‚æœè³‡æ–™å¤ªå¤§ï¼Œåªå‘Šè¨´ LLM å¤§å°ï¼Œä¸å‚³å…¨éƒ¨å…§å®¹
                                    summary_info[name] = f"DataFrame/Series with {len(val)} rows"
                            elif hasattr(val, '__len__') and len(val) < 20:
                                summary_info[name] = val

                        # --- [Step 4: é‚è¼¯åé¥‹èˆ‡ä¿®æ­£ (Logic Reflection Loop)] ---
                        status.update(label="Step 4/6: AI æ­£åœ¨æª¢æŸ¥åˆ†æçµæœçš„é‚è¼¯æ€§...")
                        
                        reflection_context = ""
                        for name, val in summary_info.items():
                            reflection_context += f"{name}: {val}\n"
                        
                        if not reflection_context:
                            reflection_context = "(ç„¡ç‰¹å®šè¼¸å‡ºè®Šæ•¸ï¼Œé€™é€šå¸¸è¡¨ç¤ºæ²’æœ‰è¨ˆç®—å‡ºä»»ä½•æ•¸æ“š)"

                        reflection_prompt = f"""
                        ä½ æ˜¯ä¸€å€‹åš´æ ¼çš„ç¨‹å¼ç¢¼å¯©æŸ¥å“¡ (QA)ã€‚
                        
                        1. ä½¿ç”¨è€…åŸå§‹å•é¡Œ: "{prompt}"
                        2. å¼·åŒ–å¾Œçš„å•é¡Œæè¿°: "{enhanced_prompt}"
                        3. ç›®å‰ç”Ÿæˆçš„ç¨‹å¼ç¢¼:
                        ```python
                        {code_to_execute}
                        ```
                        4. ç¨‹å¼åŸ·è¡Œå¾Œçš„é—œéµè®Šæ•¸çµæœ (Variable State):
                        {reflection_context}
                        5. ç¨‹å¼åŸ·è¡Œè¼¸å‡º (Stdout):
                        {execution_output}
                        
                        **ä»»å‹™ (Critical Logic Check):**
                        è«‹ä»”ç´°æª¢æŸ¥ã€Œè®Šæ•¸çµæœã€æ˜¯å¦é¡¯ç¤º**è³‡æ–™ç‚ºç©º**æˆ–**é‚è¼¯éŒ¯èª¤**ã€‚
                        
                        **åš´æ ¼åˆ¤æ–·æ¨™æº–:**
                        - âŒ **å¦‚æœè®Šæ•¸é¡¯ç¤º `Empty DataFrame`ã€`0 rows` æˆ– `[]` (ç©ºåˆ—è¡¨):** ä»£è¡¨ç¯©é¸æ¢ä»¶å¤ªåš´è‹›ã€åå­—æ‹¼éŒ¯ï¼Œæˆ–æ˜¯åˆ†æé‚è¼¯ä¸é©ç”¨æ–¼è©²è³‡æ–™å­é›†ã€‚é€™æœƒå°è‡´åœ–è¡¨ç©ºç™½ã€‚**å¿…é ˆè¦–ç‚ºå¤±æ•— (FAIL)**ã€‚
                        - âŒ **å¦‚æœ `_generated_figures_count` ç‚º 0 ä¸” `execution_output` ç‚ºç©º:** ä»£è¡¨æ²’æœ‰ç”¢ç”Ÿåœ–è¡¨ä¹Ÿæ²’æœ‰è¼¸å‡ºä»»ä½•æ–‡å­—çµæœï¼Œè¦–ç‚ºå¤±æ•—ã€‚
                        - âœ… åªè¦è³‡æ–™å­˜åœ¨ (rows > 0) ä¸” (ç”¢ç”Ÿäº†åœ–è¡¨ OR è¼¸å‡ºäº†æ–‡å­—çµæœ)ï¼Œé‚è¼¯æ­£ç¢ºå›ç­”å•é¡Œæ™‚ï¼Œå›å‚³ PASSã€‚
                        
                        **è¼¸å‡ºæ ¼å¼ (äºŒé¸ä¸€):**
                        1. å¦‚æœçµæœåˆç†ã€è³‡æ–™éç©ºä¸”æ­£ç¢ºï¼Œè«‹**åƒ…è¼¸å‡º**å­—ä¸²: "PASS"
                        2. å¦‚æœç™¼ç¾ `Empty DataFrame` æˆ–å…¶ä»–é‚è¼¯å•é¡Œï¼Œè«‹è¼¸å‡º**ä¿®æ­£å¾Œçš„å®Œæ•´ Python ç¨‹å¼ç¢¼** (å¿…é ˆåŒ…å« ```python å€å¡Š)ã€‚
                           (ä¾‹å¦‚ï¼šå˜—è©¦ä½¿ç”¨ `str.contains` é€²è¡Œæ¨¡ç³Šæœå°‹ï¼Œæ”¾å¯¬ç¯©é¸æ¢ä»¶ï¼Œæˆ–æ”¹ç”¨æ›´é©åˆè©²è³‡æ–™é‡çš„åœ–è¡¨)ã€‚
                        """
                        
                        reflection_response = client.chat.completions.create(
                            model=model_choice,
                            messages=[{"role": "user", "content": reflection_prompt}],
                            temperature=0.1
                        )
                        reflection_content = reflection_response.choices[0].message.content.strip()

                        if "PASS" not in reflection_content and "```python" in reflection_content:
                            # è§¸ç™¼é‚è¼¯ä¿®æ­£
                            status.update(label="Step 4/6: AI ç™¼ç¾è³‡æ–™ç‚ºç©ºæˆ–é‚è¼¯ç‘•ç–µï¼Œæ­£åœ¨ä¿®æ­£ç¨‹å¼ç¢¼...", state="running")
                            print(">>> Logic Refinement Triggered (Empty Data or Logic Error)")
                            
                            start = reflection_content.find("```python") + len("```python\n")
                            end = reflection_content.rfind("```")
                            new_code = reflection_content[start:end].strip()
                            
                            try:
                                plt.close('all') 
                                # é‡æ–°åˆå§‹åŒ–ç’°å¢ƒ
                                exec_globals = {
                                    "pd": pd, 
                                    "df": df.copy(), 
                                    "st": st, 
                                    "platform": platform, 
                                    "io": io, 
                                    "plt": plt,
                                    "sns": sns 
                                }
                                f = io.StringIO()
                                with redirect_stdout(f):
                                    exec(new_code, exec_globals)
                                execution_output = f.getvalue()
                                
                                code_to_execute = new_code 
                                success = True 
                                
                                summary_info = {}
                                for name, val in exec_globals.items():
                                    if name.startswith('_') or name in ignore_list: continue
                                    if isinstance(val, (int, float, str, bool)):
                                        summary_info[name] = val
                                    elif isinstance(val, (pd.DataFrame, pd.Series)):
                                         summary_info[name] = f"DataFrame/Series with {len(val)} rows"
                                    elif hasattr(val, '__len__') and len(val) < 20:
                                        summary_info[name] = val
                                        
                            except Exception as logic_fix_error:
                                print(f"Logic refinement failed: {logic_fix_error}")
                                pass

                        final_figs = [plt.figure(n) for n in plt.get_fignums()]
                        if not final_figs:
                             fig_var = exec_globals.get("fig", None)
                             if fig_var:
                                 final_figs = [fig_var]

                    # --- [Step 5: ç¢ºä¿ä¸€å®šæœ‰æ‘˜è¦è³‡è¨Š] ---
                    if not summary_info:
                        summary_info = {
                            "æç¤º": "AI æœªè¼¸å‡ºå¯ä¾›åˆ†æçš„çµ±è¨ˆè®Šæ•¸ï¼Œè«‹æ ¹æ“šåœ–è¡¨èˆ‡æå•é‚è¼¯ç”Ÿæˆæ´å¯Ÿã€‚"
                        }

                    # --- [Step 5: é¡¯ç¤ºåˆ†æå…§å®¹] ---
                    if code_to_execute:
                        with st.expander("ğŸ§¾ æŸ¥çœ‹ AI ç”Ÿæˆçš„ç¨‹å¼ç¢¼ (æœ€çµ‚ç‰ˆ)", expanded=False):
                            st.code(code_to_execute, language="python")

                    if final_figs:
                        for i, fig in enumerate(final_figs):
                            st.pyplot(fig)
                            buf = io.BytesIO()
                            fig.savefig(buf, format="png", dpi=300, bbox_inches="tight")
                            buf.seek(0)
                            st.download_button(
                                f"ğŸ“¥ ä¸‹è¼‰åœ–è¡¨ {i+1}",
                                data=buf,
                                file_name=f"ç¾½çƒåˆ†æ_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}.png",
                                mime="image/png",
                                key=f"download_new_{i}"
                            )
                    elif not execution_output:
                        st.warning("âš ï¸ AI æ²’æœ‰è¼¸å‡ºåœ–è¡¨ä¹Ÿæ²’æœ‰æ–‡å­—è¼¸å‡º (å¯èƒ½æ˜¯è³‡æ–™ç¯©é¸å¾Œç‚ºç©ºï¼Œå»ºè­°æª¢æŸ¥çƒå“¡åç¨±æ˜¯å¦æ­£ç¢º)ã€‚")

                    # --- [Step 6: ç”Ÿæˆæ•¸æ“šæ´å¯Ÿ] ---
                    status.update(label="Step 5/6: æ­£åœ¨æ’°å¯«æ•¸æ“šæ´å¯Ÿ...")
                    summary_text = ""
                    st.markdown("### ğŸ“Š æ•¸æ“šæ´å¯Ÿ")
                    
                    if execution_output:
                        st.markdown("#### ğŸ“‹ ç¨‹å¼åŸ·è¡Œçµæœ")
                        st.code(execution_output, language="text")
                        st.divider()

                    try:
                        analysis_context_str = ""
                        
                        # åŠ å…¥åŸ·è¡Œè¼¸å‡º (stdout) åˆ°åˆ†æä¸Šä¸‹æ–‡
                        if execution_output:
                            analysis_context_str += f"--- ç¨‹å¼åŸ·è¡Œè¼¸å‡º (Stdout) ---\n{execution_output}\n\n"

                        if not summary_info:
                            analysis_context_str += "AI ç¨‹å¼ç¢¼æœªç”¢ç”Ÿä»»ä½•å¯ä¾›åˆ†æçš„æ‘˜è¦è®Šæ•¸ã€‚"
                        else:
                            analysis_context_str += "ç¨‹å¼ç¢¼åŸ·è¡Œå¾Œï¼Œæ“·å–å‡ºä»¥ä¸‹æ ¸å¿ƒè®Šæ•¸èˆ‡å…¶å€¼ï¼š\n\n"
                            for name, val in summary_info.items():
                                analysis_context_str += f"### è®Šæ•¸ `{name}` (å‹åˆ¥: `{type(val).__name__}`)\n"
                                if isinstance(val, (pd.DataFrame, pd.Series)):
                                    analysis_context_str += f"```markdown\n{val.to_markdown()}\n```\n\n"
                                else:
                                    analysis_context_str += f"```\n{str(val)}\n```\n\n"
                        
                        insight_prompt = f"""
                        ä½ æ˜¯ä¸€ä½æ“æœ‰è±å¯Œç¶“é©—çš„å°ˆæ¥­ç¾½çƒæ•™ç·´ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ç²¾é€šæ•¸æ“šåˆ†æçš„æˆ°è¡“å¤§å¸«ã€‚
                        ä½¿ç”¨è€…çš„åŸå§‹å•é¡Œæ˜¯ï¼šã€Œ{prompt}ã€
                        
                        æ ¹æ“šé€™å€‹å•é¡Œï¼ŒAI ç”¢ç”Ÿä¸¦åŸ·è¡Œäº†ä¸€æ®µ Python ç¨‹å¼ç¢¼ï¼Œç¨‹å¼ç¢¼åŸ·è¡Œå¾Œç”¢ç”Ÿçš„æ ¸å¿ƒæ•¸æ“šè®Šæ•¸å¦‚ä¸‹ã€‚

                        --- æ ¸å¿ƒæ•¸æ“šè®Šæ•¸ ---
                        {analysis_context_str}
                        --- æ ¸å¿ƒæ•¸æ“šè®Šæ•¸çµæŸ ---

                        è«‹ä½ åŸºæ–¼ã€Œä½¿ç”¨è€…å•é¡Œã€å’Œä¸Šè¿°æ‰€æœ‰ã€Œæ ¸å¿ƒæ•¸æ“šè®Šæ•¸ã€ï¼Œç”¨ç¹é«”ä¸­æ–‡ç²¾ç°¡å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
                        
                        **å›ç­”é¢¨æ ¼è¦æ±‚ï¼š**
                        1.  **æ•™ç·´å£å»**ï¼šä½¿ç”¨å°ˆæ¥­ä½†æ˜“æ‡‚çš„ç¾½çƒè¡“èªï¼Œèªæ°£è¦åƒæ•™ç·´åœ¨å ´é‚ŠæŒ‡å°çƒå“¡ä¸€æ¨£ï¼Œæ—¢æœ‰æ•¸æ“šæ”¯æ’ï¼Œåˆæœ‰æˆ°è¡“æ·±åº¦ã€‚
                        2.  **å°ˆæ¥­æ´å¯Ÿ**ï¼šä¸è¦åªå”¸æ•¸å­—ï¼Œè¦è§£é‡‹æ•¸å­—èƒŒå¾Œçš„æˆ°è¡“æ„ç¾©ã€‚
                        3.  **ç²¾ç°¡æ˜ç¢º**ï¼šç›´æ¥åˆ‡å…¥é‡é»ï¼Œæä¾›å…·é«”çš„æˆ°è¡“åˆ†æã€‚
                        """
                        
                        insight = client.chat.completions.create(
                            model=model_choice,
                            messages=[
                                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­ç¾½çƒæ•™ç·´èˆ‡æ•¸æ“šæˆ°è¡“å¤§å¸«ã€‚è«‹é‡å°ä½¿ç”¨è€…å•é¡Œèˆ‡æ ¸å¿ƒæ•¸æ“šçµæœï¼Œç”¨æ•™ç·´çš„å£å»æ’°å¯«ç²¾æº–çš„æˆ°è¡“æ´å¯Ÿï¼Œæä¾›æœ‰æ·±åº¦çš„åˆ†æï¼Œéœ€ç²¾ç°¡å›ç­”ã€‚"},
                                {"role": "user", "content": insight_prompt},
                            ],
                            temperature=0.4,
                        )
                        summary_text = insight.choices[0].message.content
                        st.markdown(summary_text)

                    except Exception as e:
                        summary_text = f"*(ç„¡æ³•ç”Ÿæˆæ´å¯Ÿ: {e})*"
                        st.warning(summary_text)

                    # --- [Step 7: å„²å­˜è‡³æ­·å²] ---
                    code_block_for_history = f"```python\n{code_to_execute}\n```" if code_to_execute else ""
                    final_content_for_history = (
                        f"{code_block_for_history}\n\n"
                        f"---\n"
                        f"### ğŸ“Š æ•¸æ“šæ´å¯Ÿ\n"
                        f"{summary_text}"
                    )
                    
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": final_content_for_history.strip(),
                        "figures": final_figs,
                    })

                    status.update(label="åˆ†æå®Œæˆï¼", state="complete")

                except Exception as e:
                    status.update(label="åˆ†æå¤±æ•—", state="error")
                    st.error(f"âŒ éŒ¯èª¤: {e}")
                    st.session_state.messages.append({
                        "role": "assistant", "content": str(e), "figure": None
                    })