import streamlit as st
import os
import io
import sys
from contextlib import redirect_stdout
import platform
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
import zipfile
import matplotlib.pyplot as plt # ç¢ºä¿ matplotlib è¢«å°å…¥
import seaborn as sns # å¼•å…¥ seaborn æä¾›æ›´å¤šç¹ªåœ–é¸æ“‡ï¼Œä½†ä¸å¼·åˆ¶ä½¿ç”¨

# è‡ªè¨‚æ¨¡çµ„ (è«‹ç¢ºä¿ config/prompts.py è£¡é¢æ²’æœ‰ circular import)
from config.prompts import create_system_prompt
from utils.data_loader import load_all_data
from utils.ai_client import initialize_client

# --- åˆå§‹è¨­å®šèˆ‡ç’°å¢ƒè®Šæ•¸è¼‰å…¥ ---
load_dotenv()

# è¨­å®šé é¢
st.set_page_config(
    page_title="ç¾½çƒ AI æ•¸æ“šåˆ†æå¸«",
    page_icon="ğŸ¸",
    layout="wide"
)

# --- è¼”åŠ©å‡½æ•¸ï¼šå®‰å…¨è®€å– API Key ---
def get_api_key(key_name):
    """
    å¾ç’°å¢ƒè®Šæ•¸æˆ– Streamlit Secrets å®‰å…¨è®€å– API Keyã€‚
    """
    # å„ªå…ˆå¾ .env ç’°å¢ƒè®Šæ•¸è®€å–
    env_value = os.getenv(key_name, "")
    if env_value:
        return env_value

    # å¦‚æœç’°å¢ƒè®Šæ•¸æ²’æœ‰ï¼Œå˜—è©¦å¾ Streamlit Secrets è®€å–
    try:
        if hasattr(st, 'secrets') and st.secrets:
            return st.secrets.get(key_name, "")
    except Exception:
        pass

    return ""

# --- è¼”åŠ©å‡½æ•¸ï¼šè®€å–å ´åœ°å®šç¾© ---
@st.cache_data
def load_court_info():
    try:
        with open("court_place.txt", "r", encoding="utf-8") as f:
            return f.read()
        print("Court info loaded successfully")
    except:
        return ""

court_place_info = load_court_info()

# --- è¼”åŠ©å‡½æ•¸ï¼šç´€éŒ„ LLM äº’å‹• ---
def log_llm_interaction(step_name, messages, response_content):
    """
    å°‡ LLM çš„è¼¸å…¥èˆ‡è¼¸å‡ºç´€éŒ„åˆ°æª”æ¡ˆä¸­ï¼Œæ–¹ä¾¿é™¤éŒ¯ã€‚
    """
    log_file = "llm_debug_log.txt"
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(f"\n{'='*30}\n")
        f.write(f"[{timestamp}] Step: {step_name}\n")
        f.write(f"{'-'*30}\n")
        f.write("[Input Messages]:\n")
        for msg in messages:
            role = msg.get("role", "unknown")
            content = msg.get("content", "")
            f.write(f"  <{role.upper()}>\n{content}\n")
        
        f.write(f"\n[Output Response]:\n{response_content}\n")
        f.write(f"{'='*30}\n")

# --- è³‡æ–™è¼‰å…¥ ---
df, data_schema_info, column_definitions_info = load_all_data()

# --- Streamlit UI ---
st.title("ğŸ¸ ç¾½çƒ AI æ•¸æ“šåˆ†æå¸«")
st.markdown("#### é€éè‡ªç„¶èªè¨€ï¼Œç›´æ¥ç”Ÿæˆæ•¸æ“šåˆ†æåœ–è¡¨")

# å´é‚Šæ¬„
with st.sidebar:
    st.header("âš™ï¸ API è¨­å®š")
    api_mode = st.selectbox("API æ¨¡å¼", ["Gemini", "OpenAI å®˜æ–¹", "äº¤å¤§ä¼ºæœå™¨"], index=1)
    api_key_env_var = "GEMINI_API_KEY" if api_mode == "Gemini" else "OPENAI_API_KEY"

    # ä½¿ç”¨ get_api_key å‡½æ•¸å®‰å…¨è®€å– API Key
    default_api_key = get_api_key(api_key_env_var)

    api_key_input = st.text_input(
        f"{api_mode} API Key",
        value=default_api_key,
        type="password",
        help="ğŸ’¡ æœ¬åœ°é–‹ç™¼ï¼šå¾ .env è®€å– | é›²ç«¯éƒ¨ç½²ï¼šå¾ Streamlit Secrets è®€å–"
    )

    if api_mode == "Gemini":
        model_choice = st.selectbox("é¸æ“‡æ¨¡å‹",["gemini-2.0-flash", "gemini-1.5-pro", "gemini-1.5-flash"], index=0)
    else:
        model_choice = st.selectbox("é¸æ“‡æ¨¡å‹", ["gpt-4o-mini", "gpt-4o"], index=1)

    st.divider()

    # å¤šè¼ªå•ç­”é–‹é—œ
    enable_clarification = st.checkbox("å•Ÿç”¨å¤šè¼ªå•ç­”ï¼ˆå•é¡Œä¸æ˜ç¢ºæ™‚æœƒä¸»å‹•è©¢å•ï¼‰", value=False)

    st.divider()
    st.markdown("#### ç¯„ä¾‹å•é¡Œ")
    st.info("""
    - çƒå“¡ A çš„å„çƒç¨®åˆ†ä½ˆæ˜¯æ€éº¼æ¨£çš„ï¼Ÿè«‹ç”¨åœ“é¤…åœ–å‘ˆç¾ã€‚
    - å“ªå€‹è½é» (`landing_zone`) çš„çƒæœ€å¸¸å‡ºç¾ï¼Ÿè«‹ç”¨é•·æ¢åœ–è¡¨ç¤ºã€‚
    - å„çƒå“¡ (`player`) çš„æ®ºçƒ (`smash`) æ¬¡æ•¸æ¯”è¼ƒã€‚
    - èª°æ˜¯å¤±èª¤ç‹ï¼Ÿè«‹çµ±è¨ˆå„çƒå“¡çš„å¤±èª¤æ¬¡æ•¸ã€‚
    """)
    st.divider()

    # --- ZIP åŒ¯å‡ºåŠŸèƒ½ ---
    zip_buffer = io.BytesIO()
    has_messages = "messages" in st.session_state and st.session_state.messages
    if has_messages:
        with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zip_f:
            markdown_content = f"# ğŸ¸ ç¾½çƒ AI æ•¸æ“šåˆ†æå¸« - åˆ†æå ±å‘Š\n"
            markdown_content += f"**å„²å­˜æ™‚é–“:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n\n"
            chart_counter = 0
            for message in st.session_state.messages:
                role_emoji = "ğŸ‘¤" if message["role"] == "user" else "ğŸ¤–"
                role_title = "ä½¿ç”¨è€…æå•" if message["role"] == "user" else "AI åˆ†æå¸«å›è¦†"
                content_to_save = message["content"]
                
                # åœ¨å„²å­˜æ™‚ï¼Œå°‡ç¨‹å¼ç¢¼å€å¡Šä¿ç•™
                markdown_content += f"### {role_emoji} {role_title}\n{content_to_save.strip()}\n\n"
                
                figures = message.get("figures", [])
                if not figures and message.get("figure"):
                    figures = [message["figure"]]

                for fig in figures:
                    chart_counter += 1
                    chart_filename = f"chart_{chart_counter}.png"
                    img_buffer = io.BytesIO()
                    fig.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                    img_buffer.seek(0)
                    zip_f.writestr(chart_filename, img_buffer.getvalue())
                    markdown_content += f"![ç”¢ç”Ÿçš„åœ–è¡¨ {chart_counter}]({chart_filename})\n\n"
                markdown_content += "---\n\n"
            zip_f.writestr("åˆ†æå ±å‘Š.md", markdown_content.encode('utf-8'))

    st.download_button(
        label="ğŸ’¾ ä¸‹è¼‰åˆ†æå ±å‘Š (ZIP)",
        data=zip_buffer.getvalue(),
        file_name=f"ç¾½çƒåˆ†æå ±å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
        mime="application/zip",
        disabled=not has_messages
    )

    if st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±"):
        st.session_state.messages = []
        st.rerun()

# åˆå§‹åŒ– client èˆ‡å°è©±
client = initialize_client(api_mode, api_key_input)
if "messages" not in st.session_state:
    st.session_state.messages = []

# åˆå§‹åŒ–å¤šè¼ªå•ç­”ç‹€æ…‹
if "awaiting_clarification" not in st.session_state:
    st.session_state.awaiting_clarification = False
if "clarification_data" not in st.session_state:
    st.session_state.clarification_data = None
if "original_prompt" not in st.session_state:
    st.session_state.original_prompt = ""

# é¡¯ç¤ºæ­·å²
for idx, message in enumerate(st.session_state.messages):
    with st.chat_message(message["role"]):
        # [ä¿®æ”¹é»]ï¼šè‹¥æœ‰å„ªåŒ–å¾Œçš„æå•é‚è¼¯ï¼Œé¡¯ç¤ºåœ¨å°è©±ä¸­
        if message.get("enhanced_prompt"):
            with st.expander("ğŸ§  æŸ¥çœ‹ AI å„ªåŒ–å¾Œçš„æå•é‚è¼¯ (Step 1)", expanded=False):
                st.markdown(f"**å„ªåŒ–å°å¼• (Enhanced Prompt):**\n{message['enhanced_prompt']}")

        st.markdown(message["content"])
        figures = message.get("figures", [])
        if not figures and message.get("figure"):
            figures = [message["figure"]]

        for fig_idx, fig in enumerate(figures):
            st.pyplot(fig)
            buf = io.BytesIO()
            fig.savefig(buf, format='png', dpi=300, bbox_inches='tight')
            buf.seek(0)
            st.download_button(
                label=f"ğŸ“¥ ä¸‹è¼‰åœ–è¡¨ {fig_idx + 1}",
                data=buf,
                file_name=f"ç¾½çƒåˆ†æ_{idx}_{fig_idx}_{datetime.now().strftime('%Y%m%d')}.png",
                mime="image/png",
                key=f"download_history_{idx}_{fig_idx}",
            )

# --- ä¸»å°è©±æµç¨‹ ---
# æ·»åŠ æ­·å²ç´€éŒ„é–‹é—œ
use_history = st.toggle("ğŸ”— æ¥çºŒå‰æ–‡ (Track History)", value=False, help="é–‹å•Ÿå¾Œï¼ŒAI å°‡åƒè€ƒæœ€è¿‘çš„å°è©±ç´€éŒ„ä¾†å›ç­”å•é¡Œã€‚")

if prompt := st.chat_input("è«‹è¼¸å…¥ä½ çš„æ•¸æ“šåˆ†æå•é¡Œ..."):
    # Clear debug log
    with open("llm_debug_log.txt", "w", encoding="utf-8") as f:
        pass

    if df is None:
        st.error("âŒ æ‰¾ä¸åˆ° 'all_dataset.csv'ã€‚")
    elif not api_key_input:
        st.error("âš ï¸ è«‹è¼¸å…¥ API Keyã€‚")
    else:
        # é¡¯ç¤ºä½¿ç”¨è€…è¼¸å…¥
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # AI å›è¦†èˆ‡åŸ·è¡Œ
        with st.chat_message("assistant"):
            with st.status("AI æ­£åœ¨åˆ†æä¸¦ç”Ÿæˆåœ–è¡¨...", expanded=True) as status:
                try:
                    # 1. æº–å‚™ Prompt
                    system_prompt = create_system_prompt(data_schema_info, column_definitions_info, court_place_info)
                    
                    # ç°¡å–®çš„æ­·å²å°è©± Context (è‹¥æœ‰é–‹å•Ÿ)
                    messages = [{"role": "system", "content": system_prompt}]
                    if use_history:
                        # ç°¡å–®å–æœ€è¿‘ 4 ç­†å°è©±
                        recent = st.session_state.messages[-5:-1] if len(st.session_state.messages) > 1 else []
                        for msg in recent:
                            if msg["role"] in ["user", "assistant"] and "content" in msg:
                                messages.append({"role": msg["role"], "content": msg["content"]})
                    
                    messages.append({"role": "user", "content": prompt})

                    # 2. å‘¼å« LLM (Single Call)
                    status.write("æ­£åœ¨æ’°å¯«ç¨‹å¼ç¢¼...")
                    response = client.chat.completions.create(
                        model=model_choice,
                        messages=messages,
                        temperature=0.1 # é™ä½éš¨æ©Ÿæ€§ç¢ºä¿ç¨‹å¼ç¢¼ç©©å®š
                    )
                    ai_response = response.choices[0].message.content
                    log_llm_interaction("Single Step Analysis", messages, ai_response)

                    # 3. æå–ç¨‹å¼ç¢¼
                    code_to_execute = None
                    if "```python" in ai_response:
                        start = ai_response.find("```python") + len("```python\n")
                        end = ai_response.rfind("```")
                        code_to_execute = ai_response[start:end].strip()
                    elif "```" in ai_response: # å®¹éŒ¯
                         start = ai_response.find("```") + 3
                         end = ai_response.rfind("```")
                         code_to_execute = ai_response[start:end].strip()
                    
                    if not code_to_execute:
                         st.error("AI æœªç”Ÿæˆç¨‹å¼ç¢¼ï¼Œè«‹é‡è©¦ã€‚")
                         st.markdown(ai_response) # é¡¯ç¤ºåŸå§‹å›æ‡‰
                    else:
                        status.write("æ­£åœ¨åŸ·è¡Œç¨‹å¼ç¢¼...")
                        
                        # 4. åŸ·è¡Œç¨‹å¼ç¢¼
                        # æ¸…é™¤èˆŠåœ–
                        plt.close('all')
                        
                        exec_globals = {
                            "pd": pd, "df": df.copy(), "st": st, "platform": platform, 
                            "io": io, "plt": plt, "sns": sns
                        }
                        
                        f = io.StringIO()
                        with redirect_stdout(f):
                            try:
                                exec(code_to_execute, exec_globals)
                            except Exception as e:
                                st.error(f"ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤: {e}")
                                print(f"Execution Error: {e}")
                                # é€™è£¡ä¸å†è‡ªå‹•ä¿®å¾©ï¼Œå› ç‚ºè¦æ±‚åªèƒ½å‘¼å«ä¸€æ¬¡ LLM
                                # ä½†å¯ä»¥é¡¯ç¤ºéŒ¯èª¤ç¢¼ä¾›åƒè€ƒ
                                st.code(code_to_execute, language="python")
                                raise e # ä¸­æ–·

                        execution_output = f.getvalue()
                        
                        # 5. é¡¯ç¤ºçµæœ
                        # é¡¯ç¤ºåœ–è¡¨
                        final_figs = [plt.figure(n) for n in plt.get_fignums()]
                        if not final_figs and "fig" in exec_globals:
                             if exec_globals["fig"]:
                                 final_figs = [exec_globals["fig"]]

                        if final_figs:
                            for i, fig in enumerate(final_figs):
                                st.pyplot(fig)
                                # ä¸‹è¼‰æŒ‰éˆ•
                                buf = io.BytesIO()
                                fig.savefig(buf, format="png", dpi=300, bbox_inches="tight")
                                buf.seek(0)
                                st.download_button(
                                    f"ğŸ“¥ ä¸‹è¼‰åœ–è¡¨ {i+1}",
                                    data=buf,
                                    file_name=f"analysis_{datetime.now().strftime('%H%M%S')}_{i}.png",
                                    mime="image/png",
                                    key=f"dl_{datetime.now().timestamp()}_{i}"
                                )
                        else:
                            if execution_output:
                                st.info("åŸ·è¡Œå®Œæˆï¼Œç„¡åœ–è¡¨ç”¢å‡ºã€‚")
                            else:
                                st.warning("åŸ·è¡Œå®Œæˆï¼Œä½†ç„¡åœ–è¡¨ä¹Ÿç„¡è¼¸å‡ºã€‚")

                        # é¡¯ç¤ºæ–‡å­—è¼¸å‡º
                        if execution_output:
                            with st.expander("æŸ¥çœ‹åŸ·è¡Œè¼¸å‡º (Stdout)", expanded=True):
                                st.text(execution_output)

                        # é¡¯ç¤ºç¨‹å¼ç¢¼ (Optional)
                        with st.expander("æŸ¥çœ‹ç”Ÿæˆç¨‹å¼ç¢¼", expanded=False):
                            st.code(code_to_execute, language="python")

                        # å„²å­˜æ­·å²
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": f"åˆ†æå®Œæˆã€‚\n\n{ai_response if not final_figs else ''}", # è‹¥æœ‰åœ–å°±ä¸é‡è¤‡é¡¯ç¤ºå¤§é‡æ–‡å­—
                            "figures": final_figs
                        })

                        status.update(label="åˆ†æå®Œæˆ", state="complete")

                except Exception as e:
                    status.update(label="ç™¼ç”ŸéŒ¯èª¤", state="error")
                    st.error(f"è™•ç†å¤±æ•—: {e}")
