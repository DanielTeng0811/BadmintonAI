import streamlit as st
import os
import io
import platform
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
import zipfile

# è‡ªè¨‚æ¨¡çµ„
from config.prompts import create_system_prompt
from utils.data_loader import load_all_data
from utils.ai_client import initialize_client

# --- åˆå§‹è¨­å®šèˆ‡ç’°å¢ƒè®Šæ•¸è¼‰å…¥ ---
load_dotenv()

# è¨­å®šé é¢
st.set_page_config(
    page_title="ç¾½çƒ AI æ•¸æ“šåˆ†æå¸«",
    page_icon="ğŸ¸",
    layout="wide"
)

# --- è¼”åŠ©å‡½æ•¸ï¼šå®‰å…¨è®€å– API Key ---
def get_api_key(key_name):
    """
    å¾ç’°å¢ƒè®Šæ•¸æˆ– Streamlit Secrets å®‰å…¨è®€å– API Keyã€‚
    å„ªå…ˆé †åºï¼š
    1. ç’°å¢ƒè®Šæ•¸ (.env æª”æ¡ˆ) - æœ¬åœ°é–‹ç™¼æ™‚ä½¿ç”¨
    2. Streamlit Secrets - é›²ç«¯éƒ¨ç½²æ™‚ä½¿ç”¨
    """
    # å„ªå…ˆå¾ .env ç’°å¢ƒè®Šæ•¸è®€å–
    env_value = os.getenv(key_name, "")
    if env_value:
        return env_value

    # å¦‚æœç’°å¢ƒè®Šæ•¸æ²’æœ‰ï¼Œå˜—è©¦å¾ Streamlit Secrets è®€å–
    try:
        if hasattr(st, 'secrets') and st.secrets:
            return st.secrets.get(key_name, "")
    except Exception:
        # æœ¬åœ°é–‹ç™¼ç’°å¢ƒæ²’æœ‰ secrets.toml æ˜¯æ­£å¸¸çš„
        pass

    return ""

# --- è³‡æ–™è¼‰å…¥ ---
df, data_schema_info, column_definitions_info = load_all_data()

# --- Streamlit UI ---
st.title("ğŸ¸ ç¾½çƒ AI æ•¸æ“šåˆ†æå¸«")
st.markdown("#### é€éè‡ªç„¶èªè¨€ï¼Œç›´æ¥ç”Ÿæˆæ•¸æ“šåˆ†æåœ–è¡¨")

# å´é‚Šæ¬„
with st.sidebar:
    st.header("âš™ï¸ API è¨­å®š")
    api_mode = st.selectbox("API æ¨¡å¼", ["Gemini", "OpenAI å®˜æ–¹", "äº¤å¤§ä¼ºæœå™¨"], index=0)
    api_key_env_var = "GEMINI_API_KEY" if api_mode == "Gemini" else "OPENAI_API_KEY"

    # ä½¿ç”¨ get_api_key å‡½æ•¸å®‰å…¨è®€å– API Key
    default_api_key = get_api_key(api_key_env_var)

    api_key_input = st.text_input(
        f"{api_mode} API Key",
        value=default_api_key,
        type="password",
        help="ğŸ’¡ æœ¬åœ°é–‹ç™¼ï¼šå¾ .env è®€å– | é›²ç«¯éƒ¨ç½²ï¼šå¾ Streamlit Secrets è®€å–"
    )

    if api_mode == "Gemini":
        model_choice = st.selectbox("é¸æ“‡æ¨¡å‹",["gemini-2.0-flash", "gemini-1.5-pro", "gemini-1.5-flash"], index=0) # "gemini-2.0-flash" å¯èƒ½ä¸å­˜åœ¨, æ”¹ç‚º 1.5-flash
    else:
        model_choice = st.selectbox("é¸æ“‡æ¨¡å‹", ["gpt-4o-mini", "gpt-4o"], index=0)

    st.divider()
    st.markdown("#### ç¯„ä¾‹å•é¡Œ")
    st.info("""
    - çƒå“¡ A çš„å„çƒç¨®åˆ†ä½ˆæ˜¯æ€éº¼æ¨£çš„ï¼Ÿè«‹ç”¨åœ“é¤…åœ–å‘ˆç¾ã€‚
    - å“ªå€‹è½é» (`landing_zone`) çš„çƒæœ€å¸¸å‡ºç¾ï¼Ÿè«‹ç”¨é•·æ¢åœ–è¡¨ç¤ºã€‚
    - å„çƒå“¡ (`player`) çš„æ®ºçƒ (`smash`) æ¬¡æ•¸æ¯”è¼ƒã€‚
    - èª°æ˜¯å¤±èª¤ç‹ï¼Ÿè«‹çµ±è¨ˆå„çƒå“¡çš„å¤±èª¤æ¬¡æ•¸ã€‚
    """)
    st.divider()

    # --- ZIP åŒ¯å‡ºåŠŸèƒ½ ---
    zip_buffer = io.BytesIO()
    has_messages = "messages" in st.session_state and st.session_state.messages
    if has_messages:
        with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zip_f:
            markdown_content = f"# ğŸ¸ ç¾½çƒ AI æ•¸æ“šåˆ†æå¸« - åˆ†æå ±å‘Š\n"
            markdown_content += f"**å„²å­˜æ™‚é–“:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n\n"
            chart_counter = 0
            for message in st.session_state.messages:
                role_emoji = "ğŸ‘¤" if message["role"] == "user" else "ğŸ¤–"
                role_title = "ä½¿ç”¨è€…æå•" if message["role"] == "user" else "AI åˆ†æå¸«å›è¦†"
                content_to_save = message["content"]
                
                # åœ¨å„²å­˜æ™‚ï¼Œå°‡ç¨‹å¼ç¢¼å€å¡Šä¿ç•™
                markdown_content += f"### {role_emoji} {role_title}\n{content_to_save.strip()}\n\n"
                
                if message.get("figure") is not None:
                    chart_counter += 1
                    chart_filename = f"chart_{chart_counter}.png"
                    img_buffer = io.BytesIO()
                    message["figure"].savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                    img_buffer.seek(0)
                    zip_f.writestr(chart_filename, img_buffer.getvalue())
                    markdown_content += f"![ç”¢ç”Ÿçš„åœ–è¡¨ {chart_counter}]({chart_filename})\n\n"
                markdown_content += "---\n\n"
            zip_f.writestr("åˆ†æå ±å‘Š.md", markdown_content.encode('utf-8'))

    st.download_button(
        label="ğŸ’¾ ä¸‹è¼‰åˆ†æå ±å‘Š (ZIP)",
        data=zip_buffer.getvalue(),
        file_name=f"ç¾½çƒåˆ†æå ±å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
        mime="application/zip",
        disabled=not has_messages
    )

    if st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±"):
        st.session_state.messages = []
        st.rerun()

# åˆå§‹åŒ– client èˆ‡å°è©±
client = initialize_client(api_mode, api_key_input)
if "messages" not in st.session_state:
    st.session_state.messages = []

# é¡¯ç¤ºæ­·å²
for idx, message in enumerate(st.session_state.messages):
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "figure" in message and message["figure"] is not None:
            st.pyplot(message["figure"])
            buf = io.BytesIO()
            message["figure"].savefig(buf, format='png', dpi=300, bbox_inches='tight')
            buf.seek(0)
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰åœ–è¡¨",
                data=buf,
                file_name=f"ç¾½çƒåˆ†æ_{idx}_{datetime.now().strftime('%Y%m%d')}.png",
                mime="image/png",
                key=f"download_history_{idx}",
            )

# --- ä¸»å°è©±æµç¨‹ ---
if prompt := st.chat_input("è«‹è¼¸å…¥ä½ çš„æ•¸æ“šåˆ†æå•é¡Œ..."):
    if df is None:
        st.error("âŒ æ‰¾ä¸åˆ° 'all_dataset.csv'ã€‚")
    elif not api_key_input:
        st.error("âš ï¸ è«‹è¼¸å…¥ API Keyã€‚")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            # --- [ä¿®æ”¹]ï¼šä½¿ç”¨ st.status ä¾†é¡¯ç¤ºå¤šæ­¥é©Ÿé€²ç¨‹ ---
            with st.status("AI æ•¸æ“šåˆ†æå¸«æ­£åœ¨è™•ç†ä¸­...") as status:
                try:
                    # --- [NEW STEP 0ï¸âƒ£: è½‰åŒ–ä½¿ç”¨è€…å•é¡Œ] ---
                    status.update(label="Step 1/4: æ­£åœ¨é‡æ¸…æ‚¨çš„å•é¡Œ...")
                    
                    enhancement_system_prompt = f"""
                    ä½ æ˜¯ä¸€å€‹è¼”åŠ©ç³»çµ±ï¼Œä½ çš„ä»»å‹™æ˜¯å°‡ä½¿ç”¨è€…çš„ç°¡çŸ­æ•¸æ“šåˆ†æå•é¡Œï¼Œè½‰åŒ–ç‚ºä¸€å€‹æ›´æ¸…æ™°ã€æ›´å®Œæ•´ã€æ›´å…·é«”çš„æ•¸æ“šåˆ†æä»»å‹™æè¿°ï¼Œå¿…é ˆè€ƒæ…®ä½¿ç”¨è€…æ‰€æœ‰æ–¹é¢çš„å¯èƒ½ï¼ŒåŠæ•¸æ“šä¸­æ‰€æœ‰æ¬„ä½çš„é—œè¯æ€§ã€‚
                    é€™å€‹æè¿°å°‡è¢«äº¤çµ¦å¦ä¸€å€‹ AI (Python ç¨‹å¼ç¢¼ç”Ÿæˆå™¨) ä¾†åŸ·è¡Œã€‚
                    
                    ä½ å¿…é ˆè€ƒæ…®ä»¥ä¸‹çš„è³‡æ–™åº« schemaï¼š
                    {data_schema_info}
                    
                    ä½ çš„è¼¸å‡º**åªèƒ½**åŒ…å«è½‰åŒ–å¾Œçš„ç¹é«”ä¸­æ–‡å•é¡Œæ•˜è¿°ï¼Œä¸è¦æœ‰ä»»ä½•å‰è¨€ã€å¾Œèªæˆ–è§£é‡‹ã€‚

                    ç¯„ä¾‹ 1:
                    ä½¿ç”¨è€…è¼¸å…¥ï¼šèª°æ˜¯å¤±èª¤ç‹ï¼Ÿ
                    ä½ è¼¸å‡ºï¼šè«‹çµ±è¨ˆ 'player' æ¬„ä½ä¸­ 'type' ç‚º 'error' (å¤±èª¤) çš„æ¬¡æ•¸ï¼Œä¸¦æ‰¾å‡ºèª°çš„å¤±èª¤æ¬¡æ•¸æœ€é«˜ï¼Œä¸¦å°‡çµæœå„²å­˜åœ¨ä¸€å€‹è®Šæ•¸ä¸­ã€‚
                    
                    ç¯„ä¾‹ 2:
                    ä½¿ç”¨è€…è¼¸å…¥ï¼šçƒå“¡ A çš„åœ“é¤…åœ–
                    ä½ è¼¸å‡ºï¼šè«‹åˆ†æ 'player' æ¬„ä½ç‚º 'A' çš„æ‰€æœ‰æ“Šçƒï¼Œä¸¦ä½¿ç”¨åœ“é¤…åœ–é¡¯ç¤º 'type' (çƒç¨®) çš„åˆ†ä½ˆæ¯”ä¾‹ã€‚
                    
                    ç¯„ä¾‹ 3:
                    ä½¿ç”¨è€…è¼¸å…¥ï¼šè½é»
                    ä½ è¼¸å‡ºï¼šè«‹çµ±è¨ˆ 'landing_zone' (è½é») æ¬„ä½ä¸­æ¯å€‹å€åŸŸå‡ºç¾çš„æ¬¡æ•¸ï¼Œä¸¦ç”¨é•·æ¢åœ–é¡¯ç¤ºçµæœã€‚
                    """
                    
                    enhancement_response = client.chat.completions.create(
                        model=model_choice,
                        messages=[
                            {"role": "system", "content": enhancement_system_prompt},
                            {"role": "user", "content": prompt} # ä½¿ç”¨åŸå§‹ prompt
                        ],
                        temperature=0.2
                    )
                    enhanced_prompt = enhancement_response.choices[0].message.content.strip()
                    # --- [NEW STEP 0ï¸âƒ£ çµæŸ] ---
                    print(enhanced_prompt)
                    # Step 1ï¸âƒ£: ç”Ÿæˆåˆ†æç¨‹å¼ç¢¼
                    status.update(label="Step 2/4: æ­£åœ¨ç”Ÿæˆåˆ†æç¨‹å¼ç¢¼...")
                    system_prompt = create_system_prompt(data_schema_info, column_definitions_info)
                    # --- å°‡ system_prompt å¯«å…¥ txt æª”æ¡ˆ ---
                    file_name = "system_prompt.txt"  # æ‚¨å¸Œæœ›çš„æª”æ¡ˆåç¨±
                                
                    try:
                        # 'w' ä»£è¡¨å¯«å…¥æ¨¡å¼ (write)
                        # encoding='utf-8' ç¢ºä¿èƒ½æ­£ç¢ºè™•ç†ä¸­æ–‡å­—ç¬¦
                        with open(file_name, 'w', encoding='utf-8') as f:
                            f.write(system_prompt)
                        
                        print(f"æˆåŠŸå°‡ system_prompt å¯«å…¥æª”æ¡ˆ: {file_name}")

                    except Exception as e:
                        print(f"å¯«å…¥æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    # --- [ä¿®æ”¹]ï¼šå»ºç«‹ conversationï¼Œç”¨ enhanced_prompt æ›¿æ›æœ€å¾Œä¸€å‰‡ user è¨Šæ¯ ---
                    conversation = [{"role": "system", "content": system_prompt}]
                    # åŠ å…¥é™¤äº†æœ€å¾Œä¸€å‰‡è¨Šæ¯ä¹‹å¤–çš„æ‰€æœ‰æ­·å²
                    if len(st.session_state.messages) > 1:
                        for m in st.session_state.messages[:-1]:
                            if m.get("content"): # ç¢ºä¿ content å­˜åœ¨
                                conversation.append({"role": m["role"], "content": m["content"]})
                    
                    # æœ€å¾Œä¸€å‰‡ user è¨Šæ¯ä½¿ç”¨å¼·åŒ–å¾Œçš„ç‰ˆæœ¬
                    conversation.append({"role": "user", "content": enhanced_prompt})
                    # --- [ä¿®æ”¹çµæŸ] ---

                    response = client.chat.completions.create(
                        model=model_choice, messages=conversation
                    )
                    ai_response = response.choices[0].message.content

                    # å–å‡º Python code
                    code_to_execute = None
                    if "```python" in ai_response:
                        start = ai_response.find("```python") + len("```python\n")
                        end = ai_response.rfind("```")
                        code_to_execute = ai_response[start:end].strip()

                    # Step 2ï¸âƒ£: åŸ·è¡Œç¨‹å¼ (æ ¸å¿ƒä¿®æ”¹è™• 1)
                    status.update(label="Step 3/4: æ­£åœ¨åŸ·è¡Œç¨‹å¼ç¢¼ä¸¦ç¹ªè£½åœ–è¡¨...")
                    final_fig = None
                    summary_info = {} # æ”¹ç”¨å­—å…¸ä¾†å„²å­˜æ‰€æœ‰å°å‹è®Šæ•¸
                    if code_to_execute:
                        exec_globals = {"pd": pd, "df": df.copy(), "st": st, "platform": platform, "io": io}
                        exec(code_to_execute, exec_globals)
                        final_fig = exec_globals.get("fig", None)
                        
                        # --- ä¿®æ”¹é–‹å§‹ ---
                        # éæ­·æ‰€æœ‰åŸ·è¡Œå¾Œç”¢ç”Ÿçš„è®Šæ•¸ï¼Œæ”¶é›†å°å‹ã€é‡è¦çš„è³‡è¨Š
                        ignore_list = ['df', 'pd', 'st', 'platform', 'io', 'fig', 'np', 'plt', 'sns']
                        for name, val in exec_globals.items():
                            # å¿½ç•¥å…§å»ºè®Šæ•¸å’Œè¦æ’é™¤çš„è®Šæ•¸
                            if name.startswith('_') or name in ignore_list:
                                continue

                            # æ¢ä»¶1: æŠ“å–æ‰€æœ‰åŸºæœ¬å‹åˆ¥çš„è®Šæ•¸ (æ•¸å­—, å­—ä¸², å¸ƒæ—)
                            if isinstance(val, (int, float, str, bool)):
                                summary_info[name] = val
                            # æ¢ä»¶2: æŠ“å–é•·åº¦ < 20 çš„ list, tuple, dict, Series, DataFrame
                            elif hasattr(val, '__len__') and not isinstance(val, str) and len(val) < 20:
                                summary_info[name] = val
                        # --- ä¿®æ”¹çµæŸ ---

                    # Step 3ï¸âƒ£: ç¢ºä¿ä¸€å®šæœ‰æ‘˜è¦è³‡è¨Š (æ ¸å¿ƒä¿®æ”¹è™• 2)
                    if not summary_info: # æ”¹ç‚ºæª¢æŸ¥å­—å…¸æ˜¯å¦ç‚ºç©º
                        summary_info = {
                            "æç¤º": "AI æœªè¼¸å‡ºå¯ä¾›åˆ†æçš„çµ±è¨ˆè®Šæ•¸ï¼Œè«‹æ ¹æ“šåœ–è¡¨èˆ‡æå•é‚è¼¯ç”Ÿæˆæ´å¯Ÿã€‚"
                        }

                    # Step 4ï¸âƒ£: é¡¯ç¤ºåˆ†æå…§å®¹
                    if code_to_execute:
                        with st.expander("ğŸ§¾ æŸ¥çœ‹ AI ç”Ÿæˆçš„ç¨‹å¼ç¢¼", expanded=False):
                            st.code(code_to_execute, language="python")

                    if final_fig:
                        st.pyplot(final_fig)
                        buf = io.BytesIO()
                        final_fig.savefig(buf, format="png", dpi=300, bbox_inches="tight")
                        buf.seek(0)
                        st.download_button(
                            "ğŸ“¥ ä¸‹è¼‰åœ–è¡¨",
                            data=buf,
                            file_name=f"ç¾½çƒåˆ†æ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                            mime="image/png",
                        )
                    else:
                        st.warning("âš ï¸ AI æ²’æœ‰è¼¸å‡ºåœ–è¡¨ã€‚")

                    # Step 5ï¸âƒ£: ä¸€å®šç”Ÿæˆæ•¸æ“šæ´å¯Ÿ (æ ¸å¿ƒä¿®æ”¹è™• 3)
                    status.update(label="Step 4/4: æ­£åœ¨æ’°å¯«æ•¸æ“šæ´å¯Ÿ...")
                    summary_text = ""
                    st.markdown("### ğŸ“Š æ•¸æ“šæ´å¯Ÿ")
                    
                    try:
                        # --- ä¿®æ”¹é–‹å§‹ ---
                        # å°‡ summary_info å­—å…¸æ ¼å¼åŒ–ç‚ºçµ¦ AI çš„ prompt å­—ä¸²
                        analysis_context_str = ""
                        if not summary_info:
                            analysis_context_str = "AI ç¨‹å¼ç¢¼æœªç”¢ç”Ÿä»»ä½•å¯ä¾›åˆ†æçš„æ‘˜è¦è®Šæ•¸ã€‚"
                        else:
                            analysis_context_str += "ç¨‹å¼ç¢¼åŸ·è¡Œå¾Œï¼Œæ“·å–å‡ºä»¥ä¸‹æ ¸å¿ƒè®Šæ•¸èˆ‡å…¶å€¼ï¼š\n\n"
                            for name, val in summary_info.items():
                                analysis_context_str += f"### è®Šæ•¸ `{name}` (å‹åˆ¥: `{type(val).__name__}`)\n"
                                
                                # å° DataFrame å’Œ Series ç‰¹åˆ¥ä½¿ç”¨ markdown æ ¼å¼åŒ–
                                if isinstance(val, (pd.DataFrame, pd.Series)):
                                    analysis_context_str += f"```markdown\n{val.to_markdown()}\n```\n\n"
                                else:
                                    analysis_context_str += f"```\n{str(val)}\n```\n\n"
                        
                        # (é™¤éŒ¯ç”¨)
                        # with open("analysis_context_output.txt", "w", encoding="utf-8") as f:
                        #     f.write(analysis_context_str)
                            
                        # å»ºç«‹æ–°çš„ insight prompt
                        insight_prompt = f"""
                        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¾½çƒæ•¸æ“šåˆ†æå¸«ã€‚
                        ä½¿ç”¨è€…çš„åŸå§‹å•é¡Œæ˜¯ï¼šã€Œ{prompt}ã€
                        
                        æ ¹æ“šé€™å€‹å•é¡Œï¼ŒAI ç”¢ç”Ÿä¸¦åŸ·è¡Œäº†ä¸€æ®µ Python ç¨‹å¼ç¢¼ï¼Œç¨‹å¼ç¢¼åŸ·è¡Œå¾Œç”¢ç”Ÿçš„æ ¸å¿ƒæ•¸æ“šè®Šæ•¸å¦‚ä¸‹ã€‚

                        --- æ ¸å¿ƒæ•¸æ“šè®Šæ•¸ ---
                        {analysis_context_str}
                        --- æ ¸å¿ƒæ•¸æ“šè®Šæ•¸çµæŸ ---

                        è«‹ä½ åŸºæ–¼ã€Œä½¿ç”¨è€…å•é¡Œã€å’Œä¸Šè¿°æ‰€æœ‰ã€Œæ ¸å¿ƒæ•¸æ“šè®Šæ•¸ã€ï¼Œç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ä¸€ä»½ç²¾ç°¡ã€æ¢ç†åˆ†æ˜çš„æ•¸æ“šæ´å¯Ÿå ±å‘Šã€‚
                        å ±å‘Šæ‡‰åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š
                        1.  **ç›´æ¥å›ç­”**ï¼šç›´æ¥ä¸”æ˜ç¢ºåœ°å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
                        2.  **é—œéµç™¼ç¾**ï¼šå¾æ•¸æ“šä¸­æç…‰å‡º 1 åˆ° 3 å€‹æœ€é—œéµçš„è§€å¯Ÿæˆ–è¶¨å‹¢ï¼Œä¸¦èªªæ˜æ˜¯æ ¹æ“šå“ªäº›è®Šæ•¸å¾—å‡ºçš„çµè«–ã€‚
                        3.  **ç¸½çµ**ï¼šç”¨ä¸€å¥è©±ç¸½çµåˆ†æçµæœã€‚

                        è«‹é¿å…é‡è¤‡æè¿°æ•¸æ“šå…§å®¹ï¼Œå°ˆæ³¨æ–¼æä¾›æœ‰åƒ¹å€¼çš„è¦‹è§£ã€‚
                        """
                        # --- ä¿®æ”¹çµæŸ ---
                        
                        insight = client.chat.completions.create(
                            model=model_choice,
                            messages=[
                                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­ç¾½çƒæ•¸æ“šåˆ†æå¸«ï¼Œè«‹é‡å°ä½¿ç”¨è€…å•é¡Œèˆ‡æ ¸å¿ƒæ•¸æ“šçµæœï¼Œæ’°å¯«ç²¾æº–æ´å¯Ÿï¼Œåªæä¾›æœ‰ç”¨çš„è³‡è¨Šã€‚"},
                                {"role": "user", "content": insight_prompt},
                            ],
                            temperature=0.4,
                        )
                        summary_text = insight.choices[0].message.content
                        st.markdown(summary_text)

                    except Exception as e:
                        summary_text = f"*(ç„¡æ³•ç”Ÿæˆæ´å¯Ÿ: {e})*"
                        st.warning(summary_text)

                    # Step 6ï¸âƒ£: å„²å­˜è‡³æ­·å²
                    code_block_for_history = f"```python\n{code_to_execute}\n```" if code_to_execute else ""
                    final_content_for_history = (
                        f"{code_block_for_history}\n\n"
                        f"---\n"
                        f"### ğŸ“Š æ•¸æ“šæ´å¯Ÿ\n"
                        f"{summary_text}"
                    )
                    
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": final_content_for_history.strip(),
                        "figure": final_fig,
                    })

                    # --- [ä¿®æ”¹]ï¼šæ›´æ–° status ç‚ºå®Œæˆ ---
                    status.update(label="åˆ†æå®Œæˆï¼", state="complete")

                except Exception as e:
                    # --- [ä¿®æ”¹]ï¼šæ›´æ–° status ç‚ºéŒ¯èª¤ ---
                    status.update(label="åˆ†æå¤±æ•—", state="error")
                    st.error(f"âŒ éŒ¯èª¤: {e}")
                    st.session_state.messages.append({
                        "role": "assistant", "content": str(e), "figure": None
                    })