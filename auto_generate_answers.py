"""
å®Œå…¨è‡ªå‹•åŒ–å•ç­”è…³æœ¬ - å•é¡Œ 63-100

é€™å€‹è…³æœ¬æœƒï¼š
1. è‡ªå‹•è®€å–å•é¡Œ 63-100
2. ä½¿ç”¨ä½ çš„ BadmintonAI æ ¸å¿ƒé‚è¼¯è‡ªå‹•ç”Ÿæˆç­”æ¡ˆ
3. å°‡çµæœæ•´ç†æˆ Jupyter Notebook æ ¼å¼

ä½¿ç”¨æ–¹å¼ï¼š
    python auto_generate_answers.py
"""

# é‡è¦ï¼šå¿…é ˆåœ¨å°å…¥ matplotlib.pyplot ä¹‹å‰è¨­å®šå¾Œç«¯
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº’å‹•å¼å¾Œç«¯ï¼Œä¸æœƒå½ˆå‡ºåœ–è¡¨è¦–çª—

import os
import sys
import json
import io
import platform
from contextlib import redirect_stdout
from datetime import datetime
from dotenv import load_dotenv
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# å°å…¥ä½ çš„ BadmintonAI æ ¸å¿ƒæ¨¡çµ„
from config.prompts import create_system_prompt
from utils.data_loader import load_all_data
from utils.ai_client import initialize_client

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()


class AutoQuestionAnswerer:
    """è‡ªå‹•å•ç­”ç³»çµ±"""

    def __init__(self,
                 questions_file='test_question_modified.json',
                 output_file='question_ans_final_63to100.ipynb',
                 api_mode='OpenAI å®˜æ–¹',
                 model='gpt-4o'):

        self.questions_file = questions_file
        self.output_file = output_file
        self.api_mode = api_mode
        self.model = model

        # åˆå§‹åŒ– API
        api_key = os.getenv('OPENAI_API_KEY' if 'OpenAI' in api_mode else 'GEMINI_API_KEY')
        self.client = initialize_client(api_mode, api_key)

        # è¼‰å…¥æ•¸æ“š
        print("æ­£åœ¨è¼‰å…¥ç¾½çƒæ•¸æ“š...")
        self.df, self.data_schema_info, self.column_definitions_info = load_all_data()

        # è¼‰å…¥å ´åœ°è³‡è¨Š
        try:
            with open("court_place.txt", "r", encoding="utf-8") as f:
                self.court_place_info = f.read()
        except:
            self.court_place_info = ""

        # è¼‰å…¥å•é¡Œ
        self.questions = self._load_questions()

        # å»ºç«‹ notebook çµæ§‹
        self.notebook = self._create_notebook_structure()

    def _load_questions(self):
        """è¼‰å…¥å•é¡Œ 63-100"""
        with open(self.questions_file, 'r', encoding='utf-8') as f:
            all_questions = json.load(f)
        return [q for q in all_questions if 63 <= q['ç·¨è™Ÿ'] <= 100]

    def _create_notebook_structure(self):
        """å»ºç«‹ Jupyter Notebook åŸºæœ¬çµæ§‹"""
        return {
            "cells": [],
            "metadata": {
                "kernelspec": {
                    "display_name": "Python 3",
                    "language": "python",
                    "name": "python3"
                },
                "language_info": {
                    "codemirror_mode": {"name": "ipython", "version": 3},
                    "file_extension": ".py",
                    "mimetype": "text/x-python",
                    "name": "python",
                    "nbconvert_exporter": "python",
                    "pygments_lexer": "ipython3",
                    "version": "3.8.0"
                }
            },
            "nbformat": 4,
            "nbformat_minor": 4
        }

    def _add_markdown_cell(self, question_number, question_text):
        """æ–°å¢ markdown cellï¼ˆå•é¡Œï¼‰"""
        cell = {
            "cell_type": "markdown",
            "metadata": {},
            "source": [f"ç¬¬{question_number}é¡Œ\n", question_text]
        }
        self.notebook["cells"].append(cell)

    def _add_code_cell(self, code_text):
        """æ–°å¢ code cellï¼ˆAI ç”Ÿæˆçš„ç¨‹å¼ç¢¼ï¼‰"""
        code_lines = code_text.split('\n')
        source = [line + '\n' for line in code_lines[:-1]]
        if code_lines[-1]:
            source.append(code_lines[-1])

        cell = {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": source
        }
        self.notebook["cells"].append(cell)

    def _save_notebook(self):
        """å„²å­˜ notebook"""
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(self.notebook, f, ensure_ascii=False, indent=2)
        print(f"âœ“ Notebook å·²å„²å­˜è‡³: {self.output_file}")

    def _generate_code_for_question(self, prompt):
        """
        ä½¿ç”¨ BadmintonAI çš„é‚è¼¯ç”Ÿæˆç¨‹å¼ç¢¼
        é€™è£¡è¤‡è£½äº† front_page.py çš„æ ¸å¿ƒé‚è¼¯
        """

        # Step 1: è½‰åŒ–ä½¿ç”¨è€…å•é¡Œ
        enhancement_system_prompt = f"""
        ä½ æ˜¯è³‡æ–™åˆ†æè¼”åŠ©ç³»çµ±ã€‚è«‹åˆ†æä½¿ç”¨è€…å•é¡Œï¼š
        1. å°‡ç°¡çŸ­å•é¡Œè½‰åŒ–ç‚ºç²¾æº–å®Œæ•´çš„æ•¸æ“šåˆ†æå•é¡Œ (Enhanced Prompt)ï¼Œå‹¿éåº¦è©®é‡‹ï¼Œç”¨ç¹é«”ä¸­æ–‡ã€‚
        2. åˆ¤æ–·å•é¡Œæ˜¯å¦å¯èƒ½ç”¨åˆ°å ´åœ°è³‡è¨Šã€‚è‹¥ä¸ç¢ºå®šï¼Œè¼¸å‡ºtrue
           - è‹¥å•é¡Œå¯èƒ½éœ€è¦ç”¨åˆ°å ´åœ°è³‡è¨Šï¼šå‰å ´/ä¸­å ´/å¾Œå ´ã€ç¶²å‰/åº•ç·š/é‚Šç·šã€è½é»ã€ç«™ä½ã€å€åŸŸ (Area/Zone/Location)... -> true

        è¼¸å‡º JSON (No Markdown):
        {{
            "enhanced_prompt": "å®Œæ•´çš„å•é¡Œ",
            "needs_court_info": true/false
        }}
        """

        messages_1 = [
            {"role": "system", "content": enhancement_system_prompt},
            {"role": "user", "content": prompt}
        ]

        enhancement_response = self.client.chat.completions.create(
            model=self.model,
            messages=messages_1,
            temperature=0.2
        )

        raw_content = enhancement_response.choices[0].message.content.strip()
        enhanced_prompt = raw_content
        needs_court_info = False

        try:
            json_str = raw_content
            if "```json" in raw_content:
                start = raw_content.find("```json") + 7
                end = raw_content.rfind("```")
                json_str = raw_content[start:end].strip()
            elif "```" in raw_content:
                start = raw_content.find("```") + 3
                end = raw_content.rfind("```")
                json_str = raw_content[start:end].strip()

            parsed = json.loads(json_str)
            enhanced_prompt = parsed.get("enhanced_prompt", raw_content)
            needs_court_info = parsed.get("needs_court_info", False)
        except:
            if any(k in prompt for k in ["è½é»", "ä½ç½®", "å€åŸŸ", "åº§æ¨™", "location", "area"]):
                needs_court_info = True

        # Step 2: ç”Ÿæˆåˆ†æç¨‹å¼ç¢¼
        system_prompt = create_system_prompt(self.data_schema_info, self.column_definitions_info)

        if needs_court_info and self.court_place_info:
            system_prompt += f"\n\n**å ´åœ°ä½ç½®åƒè€ƒè³‡è¨Š (Court Grid Definitions):**\n{self.court_place_info}\n"

        system_prompt += """
        \n**æœ€ä½³å¯¦è¸:**
        1. å€åˆ†é€£çºŒæ•¸å€¼(Float)èˆ‡é¡åˆ¥ã€‚åº§æ¨™å‹¿ç›´æ¥ groupbyã€‚
        2. è»¸æ¨™ç±¤é¿å…å¤§é‡æµ®é»æ•¸ã€‚
        3. ç¹ªåœ–å‰æª¢æŸ¥ `if len(filtered_df) > 0:`ã€‚
        """

        conversation = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": enhanced_prompt}
        ]

        response = self.client.chat.completions.create(
            model=self.model,
            messages=conversation
        )

        ai_response = response.choices[0].message.content

        # å–å‡º Python code
        code_to_execute = None
        if "```python" in ai_response:
            start = ai_response.find("```python") + len("```python\n")
            end = ai_response.rfind("```")
            code_to_execute = ai_response[start:end].strip()

        return code_to_execute, conversation

    def _execute_and_fix_code(self, code_to_execute, conversation, prompt):
        """
        åŸ·è¡Œç¨‹å¼ç¢¼ä¸¦è‡ªå‹•ä¿®å¾©éŒ¯èª¤
        å®Œå…¨è¤‡è£½ front_page.py çš„ Step 3 + Step 4 é‚è¼¯
        """

        if not code_to_execute:
            return None

        # --- Step 3: åŸ·è¡Œç¨‹å¼ (Runtime Error Fix Loop) ---
        max_retries = 3
        retry_count = 0
        success = False
        last_error = None
        exec_globals = {}
        execution_output = ""
        summary_info = {}

        # è¿´åœˆ 1: è™•ç†èªæ³•/åŸ·è¡ŒéŒ¯èª¤ (Syntax/Runtime Errors)
        while retry_count <= max_retries:
            try:
                plt.close('all')

                exec_globals = {
                    "pd": pd,
                    "df": self.df.copy(),
                    "platform": platform,
                    "io": io,
                    "plt": plt,
                    "sns": sns
                }

                f = io.StringIO()
                with redirect_stdout(f):
                    exec(code_to_execute, exec_globals)

                execution_output = f.getvalue()
                success = True
                break  # æˆåŠŸåŸ·è¡Œï¼Œè·³å‡ºè¿´åœˆ

            except Exception as e:
                retry_count += 1
                last_error = e
                print(f"  âš  åŸ·è¡ŒéŒ¯èª¤ (å˜—è©¦ {retry_count}/{max_retries}): {str(e)[:100]}")

                conversation.append({"role": "assistant", "content": f"```python\n{code_to_execute}\n```"})
                error_feedback = f"åŸ·è¡Œä¸Šè¿°ç¨‹å¼ç¢¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}ã€‚è«‹ä¿®æ­£éŒ¯èª¤ä¸¦é‡æ–°è¼¸å‡ºå®Œæ•´ç¨‹å¼ç¢¼ (åŒ…å«å¿…è¦çš„ import)ã€‚"
                conversation.append({"role": "user", "content": error_feedback})

                correction_response = self.client.chat.completions.create(
                    model=self.model,
                    messages=conversation
                )

                ai_correction = correction_response.choices[0].message.content

                if "```python" in ai_correction:
                    start = ai_correction.find("```python") + len("```python\n")
                    end = ai_correction.rfind("```")
                    code_to_execute = ai_correction[start:end].strip()

        if not success:
            print(f"  âœ— ç„¡æ³•ä¿®å¾©ç¨‹å¼ç¢¼éŒ¯èª¤: {last_error}")
            return code_to_execute

        # --- æå–è®Šæ•¸ (ä¾› Step 4 é‚è¼¯æª¢æŸ¥ä½¿ç”¨) ---
        ignore_list = ['df', 'pd', 'platform', 'io', 'fig', 'np', 'plt', 'sns']

        # æª¢æŸ¥ç”Ÿæˆçš„åœ–è¡¨æ•¸é‡
        created_figs = [plt.figure(n) for n in plt.get_fignums()]
        if not created_figs and "fig" in exec_globals:
            created_figs = [exec_globals["fig"]]

        summary_info["_generated_figures_count"] = len(created_figs)

        for name, val in exec_globals.items():
            if name.startswith('_') or name in ignore_list:
                continue

            try:
                # é¿å… class ç‰©ä»¶è§¸ç™¼éŒ¯èª¤
                if isinstance(val, type):
                    continue

                if isinstance(val, (int, float, str, bool)):
                    summary_info[name] = val
                elif isinstance(val, (pd.DataFrame, pd.Series)):
                    # å¼·åˆ¶è®“ LLM çŸ¥é“è³‡æ–™æ˜¯ç©ºçš„
                    if val.empty:
                        summary_info[name] = "âš ï¸ Empty DataFrame/Series (0 rows)"
                    else:
                        summary_info[name] = f"DataFrame/Series with {len(val)} rows"
                elif hasattr(val, '__len__') and len(val) < 20:
                    summary_info[name] = val
            except Exception:
                pass

        # --- [Step 4: é‚è¼¯åé¥‹èˆ‡ä¿®æ­£ (Logic Reflection Loop)] ---
        print(f"  â†’ Step 4: æª¢æŸ¥é‚è¼¯æ­£ç¢ºæ€§...")

        reflection_context = ""
        for name, val in summary_info.items():
            reflection_context += f"{name}: {val}\n"

        if not reflection_context:
            reflection_context = "(ç„¡ç‰¹å®šè¼¸å‡ºè®Šæ•¸ï¼Œé€™é€šå¸¸è¡¨ç¤ºæ²’æœ‰è¨ˆç®—å‡ºä»»ä½•æ•¸æ“š)"

        reflection_prompt = f"""
        [æŸ¥æ ¸è³‡æ–™]
        1. å•é¡Œ: "{prompt}"
        2. ç¨‹å¼ç¢¼:
        ```python
        {code_to_execute}
        ```
        3. åŸ·è¡Œèˆ‡è®Šæ•¸: {execution_output}
        {reflection_context}

        ä½ æ˜¯åš´æ ¼çš„ã€Œç¨‹å¼ç¢¼é‚è¼¯å¯©è¨ˆå“¡ (Code Auditor)ã€ã€‚è«‹å…ˆ**é€æ­¥æ¨ç† (Chain of Thought)**ï¼Œæ‰¾å‡ºç¨‹å¼ç¢¼é‚è¼¯èˆ‡ä½¿ç”¨è€…å•é¡Œä¸ç¬¦ä¹‹è™•ï¼Œä¸¦åˆ—å‡ºå…·é«”éŒ¯èª¤ï¼Œæœ€å¾Œå†æ±ºå®šæ˜¯å¦ä¿®æ­£ã€‚
        **é‡è¦æª¢æŸ¥æ¸…å–®:**
        - ç¢ºèªç¨‹å¼ç¢¼æ˜¯å¦æœ‰æ˜ç¢ºè§£æ±ºå•é¡Œ
        - ç¢ºèªç¨‹å¼ç¢¼å…§éƒ¨é‚è¼¯æ˜¯å¦æœ‰èª¤
        - åŸ·è¡Œçµæœæ˜¯å¦åˆç†

        **é‚è¼¯éŒ¯èª¤æ¡ˆä¾‹:**
        - ğŸ› **é‚è¼¯æ½›åœ¨éŒ¯èª¤**:
            - è³‡æ–™å®Œæ•´æ€§: è®Šæ•¸æ˜¯å¦è¢«ä¸ç•¶è¦†è“‹ï¼Ÿdropna æ˜¯å¦åˆªé™¤äº†éå¤šè³‡æ–™ï¼Ÿ
            - çµ±è¨ˆæ­£ç¢ºæ€§: groupby + sum/mean/count æ˜¯å¦ç¬¦åˆé¡Œç›®èªæ„ï¼Ÿ(å¦‚ï¼šæ±‚æ¬¡æ•¸å»ç”¨ sum, æ±‚ç¸½åˆ†å»ç”¨ count)
            - æ¬„ä½é¸ç”¨: æ˜¯å¦é¸éŒ¯æ¬„ä½ï¼Ÿ (å¦‚: player A vs player B)
        - ğŸ¯ **æ„åœ–ç›¸ç¬¦æ€§**: ç¨‹å¼ç¢¼ç”¢å‡ºçš„åœ–è¡¨/æ•¸æ“šï¼Œæ˜¯å¦ç›´æ¥å›ç­”äº†ä½¿ç”¨è€…çš„å•é¡Œï¼Ÿ
        - âŒ **ç•°å¸¸æª¢æ¸¬**: æ˜¯å¦ç”¢ç”Ÿ `Empty/0 rows`ï¼Ÿåœ–è¡¨æ˜¯å¦ç©ºç™½ (`_generated_figures_count`=0)ï¼Ÿ
        - âš ï¸ **è¦–è¦ºå‘ˆç¾**:
            - åœ“é¤…åœ–: è‹¥å°æ–¼ 5% çš„é¡åˆ¥éå¤šï¼Œ**å¿…é ˆ**åˆä½µç‚ºã€Œå…¶ä»– (Others)ã€ã€‚
            - é•·æ¢åœ–: X è»¸æ¨™ç±¤è‹¥éå¤šå°è‡´æ“æ“ é›£è®€ï¼Œæ‡‰èª¿æ•´ç‚ºæ°´å¹³é•·æ¢åœ–æˆ–ç¯©é¸ Top Nã€‚
        - æ™‚é–“åºæ˜¯å¦æéŒ¯: shift()é‚è¼¯éœ€è¦ä½¿ç”¨å—?æ˜¯å¦ä½¿ç”¨æ­£ç¢º?
        **å›è¦†æ ¼å¼ (Format):**
        è«‹åš´æ ¼éµå®ˆä»¥ä¸‹æ ¼å¼å›è¦†ï¼š

        [Reasoning]
        1. (è§€å¯Ÿåˆ°çš„å•é¡Œæˆ–ç¢ºèªæ­£ç¢ºçš„äº‹å¯¦...)
        2. ...

        [Conclusion]
        (è‹¥éœ€ä¿®æ­£ï¼Œè«‹æä¾›å®Œæ•´ Python ç¨‹å¼ç¢¼ï¼ŒåŒ…å«å¿…è¦çš„ importï¼Œä¸¦å‹™å¿…ç”¨ ```python åŒ…è£¹)
        (è‹¥ç„¡éœ€ä¿®æ­£ï¼Œè«‹åƒ…å›è¦†å–®å­—: PASS)
        """

        messages_4 = [{"role": "user", "content": reflection_prompt}]
        reflection_response = self.client.chat.completions.create(
            model=self.model,
            messages=messages_4,
            temperature=0.1
        )
        reflection_content = reflection_response.choices[0].message.content.strip()

        if "```python" in reflection_content:
            # è§¸ç™¼é‚è¼¯ä¿®æ­£
            print("  â†’ Step 4: ç™¼ç¾é‚è¼¯ç‘•ç–µï¼Œæ­£åœ¨ä¿®æ­£...")

            start = reflection_content.find("```python") + len("```python\n")
            end = reflection_content.rfind("```")
            new_code = reflection_content[start:end].strip()

            try:
                plt.close('all')
                # é‡æ–°åˆå§‹åŒ–ç’°å¢ƒ
                exec_globals = {
                    "pd": pd,
                    "df": self.df.copy(),
                    "platform": platform,
                    "io": io,
                    "plt": plt,
                    "sns": sns
                }
                f = io.StringIO()
                with redirect_stdout(f):
                    exec(new_code, exec_globals)
                execution_output = f.getvalue()

                code_to_execute = new_code
                print("  âœ“ é‚è¼¯ä¿®æ­£æˆåŠŸ")

            except Exception as logic_fix_error:
                print(f"  âš  é‚è¼¯ä¿®æ­£å¤±æ•—: {logic_fix_error}ï¼Œä½¿ç”¨åŸå§‹ç¨‹å¼ç¢¼")
                # Fallback: ä½¿ç”¨åŸå§‹ç¨‹å¼ç¢¼
        else:
            print("  âœ“ é‚è¼¯æª¢æŸ¥é€šé")

        return code_to_execute

    def process_all_questions(self):
        """è™•ç†æ‰€æœ‰å•é¡Œ"""

        total = len(self.questions)
        print(f"\n{'='*70}")
        print(f"BadmintonAI è‡ªå‹•å•ç­”ç³»çµ±")
        print(f"{'='*70}")
        print(f"å•é¡Œç¯„åœ: 63-100 (å…± {total} é¡Œ)")
        print(f"API æ¨¡å¼: {self.api_mode}")
        print(f"æ¨¡å‹: {self.model}")
        print(f"{'='*70}\n")

        for idx, question in enumerate(self.questions, 1):
            q_num = question['ç·¨è™Ÿ']
            q_text = question['å•é¡Œ']

            print(f"[{idx}/{total}] å•é¡Œ {q_num}: {q_text[:50]}{'...' if len(q_text) > 50 else ''}")

            try:
                # æ–°å¢å•é¡Œåˆ° notebook
                self._add_markdown_cell(q_num, q_text)

                # ç”Ÿæˆç¨‹å¼ç¢¼
                print(f"  â†’ æ­£åœ¨ç”Ÿæˆç¨‹å¼ç¢¼...")
                code, conversation = self._generate_code_for_question(q_text)

                if not code:
                    print(f"  âš  AI æœªç”Ÿæˆç¨‹å¼ç¢¼")
                    self._add_code_cell("# AI æœªç”Ÿæˆç¨‹å¼ç¢¼")
                    continue

                # åŸ·è¡Œä¸¦ä¿®å¾©ç¨‹å¼ç¢¼ï¼ˆåŒ…å« Step 4 é‚è¼¯æª¢æŸ¥ï¼‰
                print(f"  â†’ Step 3: åŸ·è¡Œç¨‹å¼ç¢¼...")
                final_code = self._execute_and_fix_code(code, conversation, q_text)

                # åŠ å…¥åˆ° notebook
                self._add_code_cell(final_code)
                print(f"  âœ“ å®Œæˆ")

                # æ¯ 5 é¡Œè‡ªå‹•å„²å­˜
                if idx % 5 == 0:
                    self._save_notebook()
                    print(f"\n[è‡ªå‹•å„²å­˜] å·²å®Œæˆ {idx}/{total} é¡Œ\n")

            except Exception as e:
                print(f"  âœ— è™•ç†å¤±æ•—: {e}")
                self._add_code_cell(f"# è™•ç†å¤±æ•—: {e}")

            # æ¸…ç† matplotlib ç‹€æ…‹
            plt.close('all')

        # æœ€çµ‚å„²å­˜
        self._save_notebook()
        print(f"\n{'='*70}")
        print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼å…±è™•ç† {total} å€‹å•é¡Œ")
        print(f"âœ“ çµæœå·²å„²å­˜è‡³: {self.output_file}")
        print(f"{'='*70}\n")


def main():
    """ä¸»ç¨‹å¼"""

    print("\nğŸ¸ BadmintonAI è‡ªå‹•å•ç­”ç³»çµ±\n")

    # é¸æ“‡ API æ¨¡å¼
    print("è«‹é¸æ“‡ API æ¨¡å¼ï¼š")
    print("1. OpenAI å®˜æ–¹ (gpt-4o) - æ¨è–¦")
    print("2. OpenAI å®˜æ–¹ (gpt-4o-mini) - è¼ƒä¾¿å®œ")
    print("3. Gemini (gemini-2.0-flash)")

    choice = input("\nè«‹è¼¸å…¥é¸é … (1/2/3, é è¨­=1): ").strip() or "1"

    if choice == "1":
        api_mode = "OpenAI å®˜æ–¹"
        model = "gpt-4o"
    elif choice == "2":
        api_mode = "OpenAI å®˜æ–¹"
        model = "gpt-4o-mini"
    elif choice == "3":
        api_mode = "Gemini"
        model = "gemini-2.0-flash"
    else:
        print("ç„¡æ•ˆé¸é …ï¼Œä½¿ç”¨é è¨­: OpenAI å®˜æ–¹ (gpt-4o)")
        api_mode = "OpenAI å®˜æ–¹"
        model = "gpt-4o"

    print(f"\nâœ“ å·²é¸æ“‡: {api_mode} - {model}\n")

    # ç¢ºèªåŸ·è¡Œ
    confirm = input("ç¢ºå®šè¦é–‹å§‹è‡ªå‹•è™•ç† 38 å€‹å•é¡Œå—ï¼Ÿ(y/n): ").strip().lower()
    if confirm != 'y':
        print("å·²å–æ¶ˆã€‚")
        return

    # å»ºç«‹è‡ªå‹•å•ç­”ç³»çµ±
    answerer = AutoQuestionAnswerer(
        api_mode=api_mode,
        model=model
    )

    # é–‹å§‹è™•ç†
    answerer.process_all_questions()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nç¨‹å¼è¢«ä¸­æ–·ã€‚")
    except Exception as e:
        print(f"\nç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        import traceback
        traceback.print_exc()
