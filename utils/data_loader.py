"""
è³‡æ–™è¼‰å…¥ç›¸é—œå‡½æ•¸
Data loading utilities for BadmintonAI
"""
import os
import pandas as pd
import json
import io
import streamlit as st
import numpy as np

# æª”æ¡ˆè·¯å¾‘å¸¸æ•¸
DATA_FILE = "processed_data.csv"
COLUMN_DEFINITION_FILE = "column_definition.json"


@st.cache_data
def load_data(filepath):
    """
    è¼‰å…¥ CSV æ•¸æ“šä¸¦å¿«å–

    Args:
        filepath: CSV æª”æ¡ˆè·¯å¾‘

    Returns:
        pd.DataFrame or None: è¼‰å…¥çš„ DataFrameï¼Œè‹¥æª”æ¡ˆä¸å­˜åœ¨å‰‡å›å‚³ None
    """
    if os.path.exists(filepath):
        return pd.read_csv(filepath)
    return None


@st.cache_data
def get_data_schema(df):
    """
    å¾ DataFrame ç²å–æ¬„ä½å‹æ…‹è³‡è¨Šã€‚
    é¡å¤–åŠŸèƒ½ï¼š
    - è‹¥æ¬„ä½å”¯ä¸€å€¼ <= 20ï¼Œåˆ—å‡ºæ‰€æœ‰å”¯ä¸€å€¼ã€‚
    - è‹¥æ¬„ä½ç‚ºæ•¸å€¼å‹ï¼ˆint/floatï¼‰ï¼Œåˆ—å‡ºæœ€å°å€¼èˆ‡æœ€å¤§å€¼ã€‚

    Args:
        df: pandas DataFrame

    Returns:
        str: DataFrame çš„çµæ§‹è³‡è¨Šï¼ˆæ¬„ä½åç¨±ã€å‹æ…‹ã€å”¯ä¸€å€¼ã€æ•¸å€¼ç¯„åœç­‰ï¼‰
    """
    # 1. å–å¾— df.info() çš„åŸºæœ¬è³‡è¨Š
    buffer = io.StringIO()
    df.info(buf=buffer)
    schema_info = buffer.getvalue()

    # 2. å„²å­˜é¡å¤–æ¬„ä½åˆ†æè³‡è¨Š
    extra_info = []

    # 3. é€æ¬„åˆ†æ
    for col in df.columns:
        series = df[col]
        dtype = series.dtype

        # --- æ•¸å€¼æ¬„ä½çµ±è¨ˆ ---
        if np.issubdtype(dtype, np.number):  # åˆ¤æ–·æ˜¯å¦ç‚ºæ•¸å€¼å‹
            col_min = series.min(skipna=True)
            col_max = series.max(skipna=True)
            extra_info.append(f"\n### æ¬„ä½ '{col}' æ•¸å€¼ç¯„åœ:")
            extra_info.append(f"æœ€å°å€¼ = {col_min}, æœ€å¤§å€¼ = {col_max}")

        # --- å”¯ä¸€å€¼è³‡è¨Š ---
        num_unique = series.nunique()
        if num_unique <= 20:
            unique_vals = series.unique()
            unique_vals_list = list(unique_vals)
            extra_info.append(f"\n### æ¬„ä½ '{col}' (å”¯ä¸€å€¼ <= 20 å€‹):")
            extra_info.append(f"{unique_vals_list}")

    # 4. æ–°å¢å›ºå®šçš„å ´åœ°ç·¨è™Ÿå°æ‡‰è³‡è¨Š
    # court_mapping_info = (
    #     "\n" + "="*60
    #     + "\n[å ´åœ°å‰ä¸­å¾Œå ´å°æ‡‰]"
    #     + "\n" + "="*60
    #     + "\nå‰å ´: yåº§æ¨™>0.7"
    #     + "\nä¸­å ´: yåº§æ¨™åœ¨-0.1åˆ°0.7ä¹‹é–“"
    #     + "\nå¾Œå ´: yåº§æ¨™<-0.1"
    # )

    # 5. åˆä½µè¼¸å‡ºå…§å®¹ (å·²ä¿®æ­£)
    final_output = (
        schema_info
        #+ court_mapping_info # æ’å…¥å ´åœ°è³‡è¨Š
        + "\n" + "="*60  # <-- ä¿®æ­£ï¼šç§»é™¤äº†å¤šé¤˜çš„ 's'
        + "\n[æ¬„ä½é¡å¤–è³‡è¨Š (å‹•æ…‹åˆ†æ)]" # ä¿®æ”¹læ¨™é¡Œä»¥å€åˆ†
        + "\n" + "="*60
        + "".join(extra_info)
    )

    return final_output
#  åŠ å…¥"å ´åœ°ç·¨è™Ÿå°æ‡‰: å‰æ’:1-4,27,28,31,32;ä¸­æ’:5-16,26,30;å¾Œæ’:17-25,29

@st.cache_data
def load_column_definitions(filepath):
    """
    è¼‰å…¥ä¸¦æ ¼å¼åŒ–æ¬„ä½å®šç¾©ï¼ˆæ”¯æ´æ–°çš„çµæ§‹åŒ–æ ¼å¼ï¼‰

    Args:
        filepath: æ¬„ä½å®šç¾© JSON æª”æ¡ˆè·¯å¾‘

    Returns:
        str: æ ¼å¼åŒ–å¾Œçš„æ¬„ä½å®šç¾©æ–‡å­—ï¼ˆMarkdown æ ¼å¼ï¼‰
    """
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            full_definitions = json.load(f)

        # å»ºç«‹æ ¼å¼åŒ–è¼¸å‡º
        output_parts = []

        # 1. æ·»åŠ  metadata è³‡è¨Š
        if "metadata" in full_definitions:
            metadata = full_definitions["metadata"]
            output_parts.append("## æ¯”è³½è³‡æ–™çµæ§‹")
            output_parts.append(f"- æ¯”è³½å½¢å¼ï¼š{metadata.get('match_structure', {}).get('format', '')}")
            output_parts.append(f"- è¨ˆåˆ†æ–¹å¼ï¼š{metadata.get('match_structure', {}).get('set_scoring', '')}")
            if 'data_recording_note' in metadata:
                output_parts.append(f"\n{metadata['data_recording_note']}")
            output_parts.append("")

        # 2. æ·»åŠ çƒç¨®å®šç¾©
        if "shot_types" in full_definitions:
            output_parts.append("## çƒç¨®ä»£ç¢¼å°ç…§è¡¨")
            shot_types = full_definitions["shot_types"]
            for code, info in shot_types.items():
                if 'english' in info:
                    output_parts.append(f"- {code}: {info['name']} ({info['english']})")
                else:
                    output_parts.append(f"- {code}: {info['name']}")
            output_parts.append("")

        # 3. æ·»åŠ æ¬„ä½å®šç¾©ï¼ˆçµæ§‹åŒ–æ ¼å¼ï¼‰
        output_parts.append("## æ¬„ä½å®šç¾©")
        column_definitions = full_definitions.get("data_columns", [])
        for item in column_definitions:
            col_name = item.get('column', '')
            desc = item.get('description', '')

            # åŸºæœ¬è³‡è¨Š
            if 'warning' in item:
                output_parts.append(f"âš ï¸ **{item['warning']}**")
            output_parts.append(f"### `{col_name}`")
            output_parts.append(f"**èªªæ˜**ï¼š{desc}")

            # é—œéµå­—
            if 'keyword' in item:
                output_parts.append(f"- **é—œéµå­—**ï¼š{item['keyword']}")

            # è³‡æ–™é¡å‹èˆ‡å€¼åŸŸ
            if 'data_type' in item:
                output_parts.append(f"- **è³‡æ–™é¡å‹**ï¼š{item['data_type']}")
            if 'value_range' in item:
                val_range = item['value_range']
                if isinstance(val_range, list):
                    output_parts.append(f"- **å¯èƒ½å€¼**ï¼š{', '.join(val_range)}")
                else:
                    output_parts.append(f"- **å€¼åŸŸ**ï¼š{val_range}")

            # æ•¸å€¼å°æ‡‰ (New)
            if 'value_mapping' in item:
                vm = item['value_mapping']
                if isinstance(vm, dict):
                    vm_str = ", ".join([f"{k}={v}" for k,v in vm.items()])
                    output_parts.append(f"- **æ•¸å€¼å°æ‡‰**ï¼š{vm_str}")
                else:
                    output_parts.append(f"- **æ•¸å€¼å°æ‡‰**ï¼š{vm}")

            # ç¯„åœ Scope (New)
            if 'scope' in item:
                output_parts.append(f"- **ç¯„åœ**ï¼š{item['scope']}")

            # è¨ˆç®—æ–¹å¼
            if 'calculation' in item:
                output_parts.append(f"- **è¨ˆç®—æ–¹å¼**ï¼š{item['calculation']}")

            # ç¯©é¸æ¢ä»¶ (New)
            if 'filter_condition' in item:
                 output_parts.append(f"- **ç¯©é¸æ¢ä»¶**ï¼š`{item['filter_condition']}`")

            # ä½¿ç”¨æƒ…å¢ƒ
            if 'usage' in item:
                output_parts.append(f"- **ç”¨é€”**ï¼š{item['usage']}")

            # å‚™è¨» (New)
            if 'note' in item:
                output_parts.append(f"- **å‚™è¨»**ï¼š{item['note']}")

            # é‡è¦æé†’
            if 'important_note' in item:
                output_parts.append(f"- âš ï¸ **é‡è¦**ï¼š{item['important_note']}")

            # æ­£ç¢ºç”¨æ³•
            if 'correct_usage' in item:
                output_parts.append(f"- âœ… **æ­£ç¢ºç”¨æ³•**ï¼š`{item['correct_usage']}`")

        # 4. æ·»åŠ åˆ†ææŒ‡å—
        if "analysis_guidelines" in full_definitions:
            output_parts.append("\n## åˆ†ææŒ‡å—")
            guidelines = full_definitions["analysis_guidelines"]

            for guide_name, guide_info in guidelines.items():
                title_map = {
                    "rally_counting": "å›åˆè¨ˆæ•¸",
                    "winning_type_classification": "å¾—åˆ†æ‰‹æ®µåˆ†é¡ (ä¸»å‹• vs å—è¿«)",
                    "core_principles": "ğŸ’ æ ¸å¿ƒè³‡æ–™åŸå‰‡ (CORE DATA PRINCIPLES)",
                    "win_rate_calculation": "å‹ç‡è¨ˆç®—",
                    "player_name_usage": "çƒå“¡åç¨±ä½¿ç”¨",
                    "shot_type_analysis": "çƒç¨®åˆ†æ",
                    "lose_reason_filter": "å¤±èª¤åŸå› ç¯©é¸",
                    "win_reason_filter": "å¾—åˆ†æ–¹å¼ç¯©é¸",
                    "match_score_query": "æ¯”è³½æ¯”åˆ†æŸ¥è©¢",
                    "continuous_shot_query": "é€£çºŒå°æ‰“æŸ¥è©¢"
                }
                title = title_map.get(guide_name, guide_name)
                output_parts.append(f"\n### {title}")

                for key, value in guide_info.items():
                    if key == "issue":
                        output_parts.append(f"- **å•é¡Œ**ï¼š{value}")
                    elif key == "context":
                        output_parts.append(f"- **æƒ…å¢ƒ**ï¼š{value}")
                    elif key == "data_limitation":
                        output_parts.append(f"- {value}")
                    elif key == "correct_method" or key == "correct" or key == "correct_usage":
                        if isinstance(value, list):
                            output_parts.append(f"- âœ… **æ­£ç¢º**ï¼š{', '.join(value)}")
                        else:
                            output_parts.append(f"- âœ… **æ­£ç¢ºæ–¹æ³•**ï¼š`{value}`")
                    elif key == "wrong_method" or key == "wrong" or key == "wrong_usage":
                        if isinstance(value, list):
                            output_parts.append(f"- âŒ **éŒ¯èª¤**ï¼š{', '.join(value)}")
                        else:
                            output_parts.append(f"- âŒ **éŒ¯èª¤æ–¹æ³•**ï¼š`{value}`")
                    elif key.startswith("step"):
                        output_parts.append(f"- **{key.upper()}**ï¼š{value}")
                    elif key == "purpose":
                        output_parts.append(f"- **ç›®çš„**ï¼š{value}")
                    elif key == "explanation":
                        output_parts.append(f"- **èªªæ˜**ï¼š{value}")
                    elif key == "rule":
                        output_parts.append(f"- **è¦å‰‡**ï¼š{value}")
                    elif key == "example_code":
                        output_parts.append(f"- **ç¨‹å¼ç¢¼ç¯„ä¾‹**ï¼š```python\n{value}\n```")
                    elif key == "example":
                        output_parts.append(f"- **ç¯„ä¾‹**ï¼š{value}")
                    elif key == "important_note":
                        output_parts.append(f"- âš ï¸ **é‡è¦æé†’**ï¼š{value}")
                    elif key == "visualization":
                        output_parts.append(f"- **è¦–è¦ºåŒ–å»ºè­°**ï¼š{value}")
                    elif key == "note":
                        output_parts.append(f"- **å‚™è¨»**ï¼š{value}")
                    elif key == "data_format":
                        output_parts.append(f"- **è³‡æ–™æ ¼å¼**ï¼š{value}")
                    elif key == "alternative":
                        output_parts.append(f"- **æ›¿ä»£æ–¹æ¡ˆ**ï¼š{value}")
                    elif key.startswith("principle"):
                        output_parts.append(f"- ğŸ’ **{key.upper()}**ï¼š{value}")
                    elif key == "correct_implementation":
                        output_parts.append(f"- âœ… **æ­£ç¢ºå¯¦ä½œ**ï¼š`{value}`")

        return "\n".join(output_parts)

    except FileNotFoundError:
        return "éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° 'column_definition.json' æª”æ¡ˆã€‚"
    except json.JSONDecodeError:
        return "éŒ¯èª¤ï¼š'column_definition.json' æª”æ¡ˆæ ¼å¼éŒ¯èª¤ã€‚"


def load_all_data():
    """
    è¼‰å…¥æ‰€æœ‰è³‡æ–™ï¼ˆDataFrameã€Schemaã€æ¬„ä½å®šç¾©ï¼‰

    Returns:
        tuple: (df, data_schema_info, column_definitions_info)
    """
    df = load_data(DATA_FILE)

    if df is not None:
        data_schema_info = get_data_schema(df)
        column_definitions_info = load_column_definitions(COLUMN_DEFINITION_FILE)
    else:
        data_schema_info = "éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° `all_dataset.csv`ï¼Œè«‹å…ˆæº–å‚™å¥½æ•¸æ“šæª”æ¡ˆã€‚"
        column_definitions_info = load_column_definitions(COLUMN_DEFINITION_FILE)

    return df, data_schema_info, column_definitions_info
