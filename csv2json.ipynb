{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac5f53e",
   "metadata": {},
   "source": [
    "將all_dataset.csv轉換成DB，表格badminton_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67fc6961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV 已成功存入 SQLite 資料庫 data.db 的 badminton_data 表格\n",
      "前 5 筆資料：\n",
      "(1, 1, '0:07:39', 11496.0, 0.0, 0.0, 0.0, 'Kento MOMOTA', 1.0, '發短球', 0.0, 1.0, 2.0, 14.0, -0.0196492380571161, 0.3664146155198778, 1.0, 18.0, -0.073337499654345, 0.5945635355250196, None, None, 'Kento MOMOTA', None, 14.0, -0.0196492380571161, 0.3664146155198778, 0.0316904856227849, -0.1035456857115604, 10.0, -0.1632827887015297, 0.285619010735518, 0.0, 0.0, 0.0, 1.0, 0.0, 11.0, 0.0120412475656688, 0.2628689298083174, None, 0.0, 0.0, 0.0, 1, 1.0, 0)\n",
      "(1, 2, '00:07:43', 11582.0, 0.0, 0.0, 0.0, 'CHOU Tien Chen', 3.0, '長球', 0.0, 1.0, 2.0, 18.0, -0.073337499654345, 0.5945635355250196, 2.0, 32.0, -0.4513914856846306, -0.4228076653977647, '出界', '對手出界', 'Kento MOMOTA', None, 10.0, -0.1632827887015297, 0.285619010735518, 0.0, 0.0, 11.0, 0.0120412475656688, 0.2628689298083174, 0.0316904856227849, -0.1035456857115604, 0.3217714630280202, None, None, 18.0, -0.073337499654345, 0.5945635355250196, None, 0.0899452890471847, 0.3089445247895016, 0.0, 1, 1.0, 0)\n",
      "(2, 1, '0:07:55', 11881.0, 1.0, 1.0, 0.0, 'Kento MOMOTA', 1.0, '發短球', 0.0, 1.0, 2.0, 15.0, 0.0686698043876449, 0.3867684333496436, 2.0, 15.0, 0.2744344201718973, 0.4998572831495475, None, None, 'Kento MOMOTA', None, 15.0, 0.0686698043876449, 0.3867684333496436, -0.0566760594357769, -0.1111039581329691, 11.0, 0.3064756103850713, 0.2890356910072441, 0.0, 0.0, 0.0, 1.0, 0.0, 11.0, 0.011993744951868, 0.2756644752166745, None, 0.0, 0.0, 0.0, 1, 1.0, 1)\n",
      "(2, 2, '0:07:56', 11900.0, 1.0, 1.0, 0.0, 'CHOU Tien Chen', 2.0, '推撲球', 0.0, 1.0, 1.0, 15.0, 0.2744344201718973, 0.4998572831495475, 2.0, 7.0, 0.3114890567274725, -0.137782153135466, None, None, 'Kento MOMOTA', None, 11.0, 0.3064756103850713, 0.2890356910072441, -0.33252126646241, -0.3572638515004004, 11.0, 0.011993744951868, 0.2756644752166745, -0.0566760594357769, -0.1111039581329691, 0.213242541683625, 9.0, 1.0, 10.0, -0.0580868462905127, 0.1425934316491471, None, -0.032041190213174, 0.2108215921423034, 0.0, 1, 1.0, 1)\n",
      "(2, 3, '0:07:56', 11921.0, 1.0, 1.0, 0.0, 'Kento MOMOTA', 2.0, '殺球', 0.0, 0.0, 2.0, 7.0, 0.3114890567274725, -0.137782153135466, 1.0, 10.0, -0.131393895801771, 0.0929926188139302, None, None, 'Kento MOMOTA', None, 11.0, 0.011993744951868, 0.2756644752166745, -0.1529985989639092, 0.1376421933434936, 10.0, -0.0580868462905127, 0.1425934316491471, -0.33252126646241, -0.3572638515004004, 0.5105247851684769, 4.0, 9.0, 7.0, 0.1584904577635633, -0.0001399597919724, None, 0.2994953117756045, -0.4134466283521404, 0.0, 1, 1.0, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# === 1. 讀取 CSV 檔案 ===\n",
    "csv_file = \"all_dataset.csv\"   # 你的 CSV 檔名\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# === 2. 建立或連線 SQLite 資料庫 (會建立 data.db 檔案) ===\n",
    "db_file = \"data.db\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "# === 3. 將 CSV 存成一張表格 ===\n",
    "table_name = \"badminton_data\"   # 自訂表格名稱\n",
    "df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"✅ CSV 已成功存入 SQLite 資料庫 {db_file} 的 {table_name} 表格\")\n",
    "\n",
    "# === 4. 測試讀取前 5 筆資料 ===\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\"SELECT * FROM {table_name} LIMIT 5;\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "print(\"前 5 筆資料：\")\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# === 5. 關閉連線 ===\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c31981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from bitsandbytes) (2.7.0+cu128)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-win_amd64.whl (59.5 MB)\n",
      "   ---------------------------------------- 0.0/59.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.8/59.5 MB 16.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 12.6/59.5 MB 37.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 21.2/59.5 MB 44.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 29.6/59.5 MB 40.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 37.0/59.5 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 46.7/59.5 MB 40.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 56.1/59.5 MB 40.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  59.5/59.5 MB 40.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  59.5/59.5 MB 40.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  59.5/59.5 MB 40.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 59.5/59.5 MB 29.2 MB/s eta 0:00:00\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25, bitsandbytes\n",
      "Successfully installed bitsandbytes-0.48.1 rank_bm25-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U bitsandbytes rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e65b9aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.26.0\n",
      "  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from accelerate>=0.26.0) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from accelerate>=0.26.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from accelerate>=0.26.0) (2.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from accelerate>=0.26.0) (0.35.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from accelerate>=0.26.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate>=0.26.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danie\\anaconda3\\envs\\cotable\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.1.31)\n",
      "Using cached accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"accelerate>=0.26.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f76784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\danie\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.9.18-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 3.1/12.0 MB 20.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 22.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 23.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 19.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "   ---------------------------------------- 0.0/564.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 564.3/564.3 kB 9.7 MB/s eta 0:00:00\n",
      "Downloading regex-2025.9.18-cp311-cp311-win_amd64.whl (276 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 25.7 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.35.3 regex-2025.9.18 safetensors-0.6.2 tokenizers-0.22.1 transformers-4.57.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts hf.exe, huggingface-cli.exe and tiny-agents.exe are installed in 'C:\\Users\\Danie\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts transformers-cli.exe and transformers.exe are installed in 'C:\\Users\\Danie\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "042b6a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "     ---------------------------------------- 0.0/19.2 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.8/19.2 MB 20.2 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 10.5/19.2 MB 34.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 18.1/19.2 MB 35.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 19.2/19.2 MB 29.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314527 sha256=f99da0e3640b674202bffe474bbff94c9779a7596ff3a2f0127f0cd94e4d8289\n",
      "  Stored in directory: c:\\users\\danie\\appdata\\local\\pip\\cache\\wheels\\ac\\60\\cf\\538a1f183409caf1fc136b5d2c2dee329001ef6da2c5084bef\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d1e9b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow<2.21,>=2.20 (from tf-keras)\n",
      "  Downloading tensorflow-2.20.0-cp311-cp311-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.71.0)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow<2.21,>=2.20->tf-keras)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow<2.21,>=2.20->tf-keras)\n",
      "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.1.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.8)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (11.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 1.6/1.7 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading tensorflow-2.20.0-cp311-cp311-win_amd64.whl (331.8 MB)\n",
      "   ---------------------------------------- 0.0/331.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 9.7/331.8 MB 46.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 19.9/331.8 MB 48.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 30.1/331.8 MB 49.0 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 40.4/331.8 MB 49.3 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 49.0/331.8 MB 47.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 59.2/331.8 MB 48.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 70.0/331.8 MB 48.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 81.3/331.8 MB 48.9 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 91.8/331.8 MB 48.8 MB/s eta 0:00:05\n",
      "   ------------ -------------------------- 103.0/331.8 MB 49.1 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 111.9/331.8 MB 48.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------ 123.2/331.8 MB 48.9 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 133.2/331.8 MB 48.6 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 145.5/331.8 MB 49.2 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 154.9/331.8 MB 48.8 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 165.2/331.8 MB 48.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 176.9/331.8 MB 49.2 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 186.4/331.8 MB 49.2 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 199.0/331.8 MB 49.3 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 209.2/331.8 MB 49.2 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 220.7/331.8 MB 49.3 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 230.7/331.8 MB 49.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 242.7/331.8 MB 49.6 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 253.2/331.8 MB 49.5 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 262.7/331.8 MB 49.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 273.9/331.8 MB 49.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 285.2/331.8 MB 49.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 294.9/331.8 MB 49.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 304.6/331.8 MB 49.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 313.8/331.8 MB 49.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  323.5/331.8 MB 49.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.6/331.8 MB 48.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 331.8/331.8 MB 29.8 MB/s eta 0:00:00\n",
      "Downloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 35.8 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------  5.5/5.5 MB 47.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 24.0 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard, keras, tensorflow, tf-keras\n",
      "Successfully installed keras-3.11.3 tensorboard-2.20.0 tensorflow-2.20.0 tf-keras-2.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\Danie\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe and toco.exe are installed in 'C:\\Users\\Danie\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2dd1e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\danie\\appdata\\roaming\\python\\python311\\site-packages (from sentence_transformers) (4.57.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from sentence_transformers) (2.7.0+cu128)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from sentence_transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\danie\\appdata\\roaming\\python\\python311\\site-packages (from sentence_transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from sentence_transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from sentence_transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\danie\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\danie\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\danie\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\danie\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.4.26)\n",
      "Downloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-5.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3e7bf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\danie\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from accelerate) (2.7.0+cu128)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\danie\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (0.35.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\danie\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.4.26)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts accelerate-config.exe, accelerate-estimate-memory.exe, accelerate-launch.exe, accelerate-merge-weights.exe and accelerate.exe are installed in 'C:\\Users\\Danie\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b0d3b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Danie\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 載入所需的套件（Packages）\n",
    "# Import necessary libraries\n",
    "\n",
    "import json                         # 處理 JSON 格式資料的標準套件 / Standard JSON library\n",
    "import torch                        # PyTorch 是一個深度學習框架 / PyTorch deep learning framework\n",
    "from typing import List             # 用來註解 List 型別 / Type annotation for list\n",
    "from transformers import (          # Transformers 模型相關函式庫 / HuggingFace Transformers library\n",
    "    AutoModelForCausalLM,           # 自動載入因果語言模型 / Auto loader for causal language models\n",
    "    AutoTokenizer,                  # 自動載入對應的 tokenizer / Auto loader for tokenizer\n",
    "    BitsAndBytesConfig,             # 用來設定量化模型的參數 / Configuration for model quantization\n",
    "    pipeline                        # 提供簡單的模型推論介面 / Easy interface for model inference\n",
    ")\n",
    "from tqdm.autonotebook import tqdm  # 顯示進度條（適用於 Jupyter Notebook） / Progress bar in Jupyter\n",
    "import jieba                        # 中文斷詞工具 / Chinese word segmentation\n",
    "from rank_bm25 import BM25Okapi     # BM25 搜尋模型 / BM25 retrieval model\n",
    "from sentence_transformers import SentenceTransformer  # 向量模型 / Sentence embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc99263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用裝置: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Exception in thread Thread-4 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\AI\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "  File \"<frozen codecs>\", line 322, in decode\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa4 in position 7: invalid start byte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82cde8971f58427ba07418e42c295b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain how attention mechanism works in transformers.\n",
      "\n",
      "Let's break down how the attention mechanism works in Transformer models. It's a critical component that's responsible for enabling transformers to understand the relationships between different parts of an input sequence, allowing them to effectively handle long-range dependencies and contextualize information.\n",
      "\n",
      "**1. The Problem with Traditional Sequential Models (RNNs)**\n",
      "\n",
      "Traditional sequence models like Recurrent Neural Networks (RNNs) process sequences one element at a time. This creates a bottleneck because:\n",
      "\n",
      "* **Vanishing/Exploding Gradients:**  Long sequences make it difficult for the network to remember information from earlier parts of the sequence, especially when dealing with long-range dependencies.\n",
      "* **Sequential Processing:** RNNs have to process each element in order, which limits parallelization and makes them slower.\n",
      "\n",
      "**2. The Core Idea of Attention**\n",
      "\n",
      "The attention mechanism addresses these problems by allowing the model to *focus* on different parts of the input sequence when processing each element.  Instead of relying on a hidden state that has to compress all the information, attention provides a way to directly “attend” to relevant pieces of the input.\n",
      "\n",
      "**3. How Attention Works (Detailed Breakdown - Self-Attention)**\n",
      "\n",
      "Transformers primarily use *self-attention* (also sometimes called intra-attention). This means the attention mechanism operates within a single sequence –  the input sequence itself.  Here's the process:\n",
      "\n",
      "* **A. Input Embedding:**\n",
      "   - Each word (or subword) in the input sequence is converted into a numerical vector called an embedding. These embeddings represent the meaning of the word in a specific space.\n",
      "\n",
      "* **B.  Queries, Keys, and Values:**\n",
      "   - This is where the \"magic\" happens. Each input embedding is transformed into three different vectors:\n",
      "      - **Query (Q):**  Represents what you're *looking for* – a question you're asking about the input.\n",
      "      - **Key (K):** Represents what you *have* – a potential answer to your question.  Each word in the input has a corresponding key.\n",
      "      - **Value (V):**  Represents the actual *content* of the word.\n",
      "\n",
      "   - These transformations are done using learned weight matrices (linear transformations) applied to the original embeddings.  Essentially, multiplying the input embedding by these different matrices produces Q, K, and V.\n",
      "\n",
      "* **C. Calculating Attention Scores:**\n",
      "   - For each word in the sequence (as represented by\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "# ====== 設定量化參數，減少記憶體使用 / Set quantization settings for smaller memory usage ======\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # 使用 4-bit 權重 / Use 4-bit weights\n",
    "    bnb_4bit_use_double_quant=True,       # 啟用雙重量化 / Enable double quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",            # 使用 nf4 量化類型 / Quantization type\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # 使用 bfloat16 進行計算 / Use bfloat16 for compute\n",
    ")\n",
    "\n",
    "# ====== 檢查 GPU ======\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"使用裝置: {device}\")\n",
    "\n",
    "# ====== 載入微調好的語言模型（Gemma） / Load pretrained LLM ======\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"unsloth/gemma-3-4b-it\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",             # 自動分配模型到 GPU（支援多 GPU）\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# ====== 載入對應的 tokenizer / Load tokenizer for the model ======\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/gemma-3-4b-it\")\n",
    "\n",
    "# ====== 使用 pipeline 包裝模型推論介面 / Create a pipeline for generation ======\n",
    "llm_pipe = pipeline(\n",
    "    \"text-generation\",            # 任務為文本生成 / Task type\n",
    "    model=llm,                    # 使用的模型 / LLM\n",
    "    tokenizer=tokenizer,          # tokenizer\n",
    "    max_new_tokens=512,           # 回應最大長度 / Maximum new tokens\n",
    "    do_sample=False,              # 不使用隨機 sampling（使用貪婪解碼）/ Greedy decoding\n",
    "    temperature=None,\n",
    "    top_p=None,\n",
    "    top_k=None,\n",
    "    device_map=\"auto\",            # 自動使用 GPU / Automatically use GPU\n",
    ")\n",
    "\n",
    "# ====== 測試生成 ======\n",
    "prompt = \"Explain how attention mechanism works in transformers.\"\n",
    "outputs = llm_pipe(prompt)\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31415685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'column': 'rally', 'definition': '該局（set）中的第幾回合，從1開始。'}, {'column': 'ball_round', 'definition': '在當次回合中，第幾次擊球（第幾拍）。'}, {'column': 'time', 'definition': '事件的時間標記(格式:hh:mm::ss)'}, {'column': 'frame_num', 'definition': '影片的幀數或影格索引'}, {'column': 'score_status', 'definition': '得分狀態標籤'}, {'column': 'player_score', 'definition': '該球員當前的分數'}, {'column': 'opponent_score', 'definition': '對方球員當前的分數'}, {'column': 'player', 'definition': '球員姓名'}, {'column': 'server', 'definition': '發球方標籤'}, {'column': 'type', 'definition': '擊球類型'}, {'column': 'aroundhead', 'definition': '是否為頭頂球'}, {'column': 'backhand', 'definition': '是否反手擊球'}, {'column': 'hit_height', 'definition': '擊球的高度等級'}, {'column': 'hit_area', 'definition': '擊球的位置區塊'}, {'column': 'hit_x', 'definition': '擊球的X分量'}, {'column': 'hit_y', 'definition': '擊球的Y分量'}, {'column': 'landing_height', 'definition': '落點高度'}, {'column': 'landing_area', 'definition': '落點的位置區塊'}, {'column': 'landing_x', 'definition': '落點X分量'}, {'column': 'landing_y', 'definition': '落點Y分量'}, {'column': 'lose_reason', 'definition': '輸球原因'}, {'column': 'win_reason', 'definition': '贏球原因'}, {'column': 'getpoint_player', 'definition': '該回合得分的玩家'}, {'column': 'flaw', 'definition': '輸球'}, {'column': 'player_location_area', 'definition': '球員所在區域'}, {'column': 'player_location_x', 'definition': '球員X位置'}, {'column': 'player_location_y', 'definition': '球員Y位置'}, {'column': 'player_move_x', 'definition': '球員X方向移動距離'}, {'column': 'player_move_y', 'definition': '球員Y方向移動距離'}, {'column': 'opponent_location_area', 'definition': '對手所在區域'}, {'column': 'opponent_location_x', 'definition': '對手X位置'}, {'column': 'opponent_location_y', 'definition': '對手Y位置'}, {'column': 'opponent_move_x', 'definition': '對手X方向移動距離'}, {'column': 'opponent_move_y', 'definition': '對手Y方向移動距離'}, {'column': 'ball_distance', 'definition': '球路徑距離'}, {'column': 'player_type', 'definition': '球員身分類型'}, {'column': 'opponent_type', 'definition': '對手身分類型'}, {'column': 'player_move_area', 'definition': '球員移動區域代碼'}, {'column': 'moving_x', 'definition': '球員移動的X方向'}, {'column': 'moving_y', 'definition': '球員移動的Y方向'}, {'column': 'landing_court_number', 'definition': '球員速度'}, {'column': 'ball_distance_x', 'definition': '球在 X 軸方向上的移動距離'}, {'column': 'ball_distance_y', 'definition': '球在 Y 軸方向上的移動距離'}, {'column': 'db', 'definition': '是否有加入DB'}, {'column': 'set', 'definition': '當下是第幾局'}, {'column': 'match_id', 'definition': '每場比賽的唯一識別碼，用於區分不同場次。'}, {'column': 'rally_id', 'definition': '每個回合在全部數據集中的唯一識別碼。'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# === 2️⃣ 載入欄位定義 JSON 檔 ===\n",
    "with open(\"column_name.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    column_definitions = json.load(f)\n",
    "\n",
    "# Add a print statement to inspect the structure\n",
    "print(type(column_definitions))\n",
    "print(column_definitions)\n",
    "\n",
    "# === 3️⃣ 將欄位定義轉成文字說明 ===\n",
    "column_info_text = \"\\n\".join(\n",
    "    [f\"- {item['column']}: {item['definition']}\" for item in column_definitions]\n",
    ")\n",
    "\n",
    "# === 4️⃣ 將欄位定義加入系統提示中 ===\n",
    "EXTENDED_SYSTEM_PROMPT = f\"\"\"\n",
    "你是一位羽球運動與資料分析專家。\n",
    "以下是資料表的欄位名稱與定義說明，請參考這些定義回答問題：\n",
    "\n",
    "{column_info_text}\n",
    "\n",
    "請根據提問，撰寫SQL代碼，讓使用者可以在SQLite上進行查詢，避免與問題無關的指令。\n",
    "**important** 只給SQL代碼\n",
    "\"\"\".strip()\n",
    "\n",
    "# === 5️⃣ 定義使用者提問模板 ===\n",
    "USER_PROMPT = \"\"\"\n",
    "### 問題\n",
    "{query}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc7870b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```sql\n",
      "SELECT\n",
      "  score_status\n",
      "FROM data\n",
      "WHERE\n",
      "  rally = 1 AND\n",
      "  set = 1\n",
      "LIMIT 1;\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "query = \"查詢第一局第一回合的最終比數\"\n",
    "chats = [\n",
    "        {'role': \"system\", 'content': EXTENDED_SYSTEM_PROMPT},\n",
    "        {'role': \"user\", 'content': USER_PROMPT.format(query=query)},\n",
    "    ]\n",
    "response = llm_pipe(chats)[0]['generated_text'][-1]['content'].strip()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234985f",
   "metadata": {},
   "source": [
    "使用RAG給定預先設計好的簡單問題及對應代碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c225b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "# 讀取 badminton.json\n",
    "with open(\"badminton_query_templates.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 將 question 當作 chunk\n",
    "chunks = [item[\"question\"] for item in data]\n",
    "sql_template = [item[\"sql_template\"] for item in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49b6c80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Danie\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.358 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# 對每個 chunk 斷詞\n",
    "tokenized_chunks = [list(jieba.cut(chunk)) for chunk in chunks]\n",
    "\n",
    "# 建立 BM25 模型\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d06f7677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bca21021c444ae8161d2e2d9d65d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# 載入多語言 embedding 模型\n",
    "embedding_model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "# 計算每個 chunk 的向量\n",
    "chunks_embeddings = embedding_model.encode(chunks, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff36010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_retrieve(query: str, chunks: List[str], bm25: BM25Okapi) -> List[str]:\n",
    "    tokenized_query = list(jieba.cut(query))\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    rank = sorted(zip(chunks, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [chunk for chunk, _ in rank]\n",
    "\n",
    "def embedding_retrieve(query: str, chunks: List[str], chunks_embeddings: np.ndarray, embedding_model: SentenceTransformer) -> List[str]:\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    # 計算餘弦相似度\n",
    "    scores = np.dot(chunks_embeddings, query_embedding) / (np.linalg.norm(chunks_embeddings, axis=1) * np.linalg.norm(query_embedding))\n",
    "    rank = sorted(zip(chunks, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [chunk for chunk, _ in rank]\n",
    "\n",
    "def reciprocal_rank_fusion(*ranked_lists, k=60) -> List[str]:\n",
    "    scores = {}\n",
    "    for rl in ranked_lists:\n",
    "        for rank, doc in enumerate(rl, start=1):\n",
    "            scores[doc] = scores.get(doc, 0.0) + 1.0 / (k + rank)\n",
    "    fused = sorted(scores.items(), key=lambda x: -x[1])\n",
    "    return [doc for doc, _ in fused]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e850d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 這位選手在整場比賽中總共失誤幾次？\n"
     ]
    }
   ],
   "source": [
    "query = \"這位選手打甚麼球種？\"\n",
    "\n",
    "# 各檢索結果\n",
    "bm25_ranked = bm25_retrieve(query, chunks, bm25)\n",
    "embedding_ranked = embedding_retrieve(query, chunks, chunks_embeddings, embedding_model)\n",
    "\n",
    "# 融合\n",
    "fused_ranked = reciprocal_rank_fusion(bm25_ranked, embedding_ranked)\n",
    "\n",
    "print(\"Top 1:\", fused_ranked[0])\n",
    "\n",
    "top1 = fused_ranked[0]\n",
    "top1_idx = chunks.index(top1)\n",
    "similarity_score = np.dot(chunks_embeddings[top1_idx], embedding_model.encode(query)) / \\\n",
    "                   (np.linalg.norm(chunks_embeddings[top1_idx]) * np.linalg.norm(embedding_model.encode(query)))\n",
    "\n",
    "if similarity_score < 0.95:\n",
    "    print(\"無匹配\")\n",
    "else:\n",
    "    print(\"匹配到類別:\", sql_template[top1_idx])\n",
    "    print(similarity_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead0427",
   "metadata": {},
   "source": [
    "若不匹配詢問小型LLM是否可以只用SELECT完成撰寫程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb837cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 LLM 輸出：\n",
      "```sql\n",
      "SELECT final_score FROM badminton_data WHERE set = 1 AND round = 1\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# --- 定義 System Prompt ---\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that ONLY writes simple SQL SELECT statements \n",
    "for questions about the \"badminton_data\".\n",
    "以下是資料表的欄位名稱與定義說明，請參考這些定義回答問題：\n",
    "\n",
    "{column_info_text}\n",
    "If the question cannot be answered using a single simple SELECT statement ,\n",
    "respond exactly with: \"I can not help you.\" Otherwise, just write the simple SQL SELECT code and you must only use the column name of the data.\n",
    "\"\"\".strip()\n",
    "\n",
    "# --- 定義使用者 Prompt 模板 ---\n",
    "USER_PROMPT = \"\"\"\n",
    "### 問題\n",
    "{query}\n",
    "\"\"\".strip()\n",
    "\n",
    "# --- 假設使用者輸入 ---\n",
    "query = \"查詢第一局第一回合的最終比數\"\n",
    "\n",
    "# --- 建立聊天格式 ---\n",
    "chats = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT.format(query=query)}\n",
    "]\n",
    "\n",
    "# --- 呼叫小模型（Gemma-3-4B-it） ---\n",
    "response = llm_pipe(chats)[0]['generated_text'][-1]['content'].strip()\n",
    "\n",
    "# --- 輸出結果 ---\n",
    "print(\"🧩 LLM 輸出：\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
